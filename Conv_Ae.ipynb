{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66256, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72600, 8)\n(13200, 8)\n(100, 656, 8)\n(17, 656, 8)\n(10, 6600, 8)\n(1, 6600, 8)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras import Model\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Sequential\n",
    "import keras as kr\n",
    "import keras.losses as losses\n",
    " \n",
    " \n",
    "# Setup the network parameters:\n",
    "intermediate_dim = 300\n",
    "latent_dim = 10\n",
    "beta = 5\n",
    "n_sequences = 200\n",
    "n_features = 8\n",
    "\n",
    "boat_csv = pd.read_csv(\"Data/Boat_nominal_data/Boat_sequences.csv\")\n",
    "boat_csv = boat_csv.drop(columns=[\"Unnamed: 0\"])\n",
    "scaler = StandardScaler()\n",
    "normal_data = scaler.fit_transform(boat_csv)\n",
    "print(normal_data.shape)\n",
    "\n",
    "boat_val = pd.read_csv(\"Data/Boat_nominal_data/Boat_nom_validation.csv\")\n",
    "boat_val = boat_val.drop(columns=[\"Unnamed: 0\"])\n",
    "scaler = StandardScaler()\n",
    "val_nom_data = scaler.fit_transform(boat_val)\n",
    "\n",
    "\n",
    "boat_anom_csv = pd.read_csv(\"Data/Boat_anomalous_big.csv\")\n",
    "boat_anom_csv= boat_anom_csv.drop(columns=[\"Unnamed: 0\"])    \n",
    "scaler = StandardScaler()\n",
    "anomalous_data = scaler.fit_transform(boat_anom_csv)\n",
    "print(anomalous_data.shape)\n",
    "\n",
    "boat_anom_val_csv = pd.read_csv(\"Data/Boat_anomalous_validation.csv\")\n",
    "boat_anom_val_csv= boat_anom_val_csv.drop(columns=[\"Unnamed: 0\"])    \n",
    "scaler = StandardScaler()\n",
    "anomalous_val_data = scaler.fit_transform(boat_anom_val_csv)\n",
    "print(anomalous_val_data.shape)\n",
    "\n",
    "def prepare_sequences(data, batch_size, interval):\n",
    "    samples = []\n",
    "    for i in range(0,data.shape[0]- batch_size, interval):\n",
    "        sample = data[i:i+batch_size]\t\n",
    "        samples.append(sample)\n",
    "\n",
    "    sequences = np.array(samples)\n",
    "\n",
    "    # Batch size (Number of samples time steps and number of features\n",
    "    trainX = np.reshape(sequences, (len(sequences), batch_size, n_features))\n",
    "\n",
    "    return trainX\n",
    "\n",
    "\n",
    "def prepare_data():    \n",
    "    trainX_nominal = prepare_sequences(normal_data,656,656) \n",
    "    print(trainX_nominal.shape)\n",
    "    \n",
    "    valX_nominal = prepare_sequences(val_nom_data, 656,656)\n",
    "    print(valX_nominal.shape)\n",
    "    \n",
    "    trainX_anomalous = prepare_sequences(anomalous_data,6600,6600)\n",
    "    print(trainX_anomalous.shape)  \n",
    "    \n",
    "    valX_anom = prepare_sequences(anomalous_val_data,6600,6600)\n",
    "    print(valX_anom.shape)\n",
    "\n",
    "    return trainX_nominal, valX_nominal, trainX_anomalous, valX_anom\n",
    "\n",
    "\n",
    "\n",
    "trainX_nominal, valX_nominal, trainX_anomalous, valX_anom = prepare_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nConv1 (Conv1D)               (None, 6600, 300)         19500     \n_________________________________________________________________\nMaxpool1 (MaxPooling1D)      (None, 3300, 300)         0         \n_________________________________________________________________\nConv2 (Conv1D)               (None, 3300, 200)         480200    \n_________________________________________________________________\nMaxpool2 (MaxPooling1D)      (None, 1650, 200)         0         \n_________________________________________________________________\nConv3 (Conv1D)               (None, 1650, 150)         240150    \n_________________________________________________________________\nMaxpool3 (MaxPooling1D)      (None, 825, 150)          0         \n_________________________________________________________________\nConv4 (Conv1D)               (None, 825, 100)          120100    \n_________________________________________________________________\nMaxpool4 (MaxPooling1D)      (None, 165, 100)          0         \n_________________________________________________________________\nConv5 (Conv1D)               (None, 165, 50)           40050     \n_________________________________________________________________\nMaxpool5 (MaxPooling1D)      (None, 33, 50)            0         \n_________________________________________________________________\nConv6 (Conv1D)               (None, 33, 10)            4010      \n_________________________________________________________________\nMaxpool6 (MaxPooling1D)      (None, 11, 10)            0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 110)               0         \n_________________________________________________________________\nembedding (Dense)            (None, 10)                1110      \n=================================================================\nTotal params: 905,120\nTrainable params: 905,120\nNon-trainable params: 0\n_________________________________________________________________\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 6600)              72600     \n_________________________________________________________________\nreshape_1 (Reshape)          (None, 33, 200)           0         \n_________________________________________________________________\nconv-decode1 (Conv1D)        (None, 33, 8)             12808     \n_________________________________________________________________\nupsampling1 (UpSampling1D)   (None, 165, 8)            0         \n_________________________________________________________________\nconv-decode2 (Conv1D)        (None, 165, 50)           3250      \n_________________________________________________________________\nupsampling2 (UpSampling1D)   (None, 825, 50)           0         \n_________________________________________________________________\nconv-decode3 (Conv1D)        (None, 825, 100)          40100     \n_________________________________________________________________\nupsampling3 (UpSampling1D)   (None, 3300, 100)         0         \n_________________________________________________________________\nconv-decode4 (Conv1D)        (None, 3300, 8)           6408      \n_________________________________________________________________\nupsampling4 (UpSampling1D)   (None, 6600, 8)           0         \n=================================================================\nTotal params: 135,166\nTrainable params: 135,166\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 1 samples\nEpoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 56s 6s/step - loss: 1.0003 - acc: 0.1332 - val_loss: 1.2176 - val_acc: 0.3074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 400ms/step - loss: 1.2304 - acc: 0.3116 - val_loss: 0.8810 - val_acc: 0.3512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 408ms/step - loss: 0.8825 - acc: 0.3486 - val_loss: 0.8859 - val_acc: 0.3097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 414ms/step - loss: 0.8864 - acc: 0.3041 - val_loss: 0.7872 - val_acc: 0.3694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 433ms/step - loss: 0.7880 - acc: 0.3658 - val_loss: 0.8017 - val_acc: 0.3930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 412ms/step - loss: 0.8083 - acc: 0.3872 - val_loss: 0.6544 - val_acc: 0.4414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 409ms/step - loss: 0.6561 - acc: 0.4415 - val_loss: 0.6242 - val_acc: 0.4614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 401ms/step - loss: 0.6242 - acc: 0.4648 - val_loss: 0.6009 - val_acc: 0.5112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 401ms/step - loss: 0.6046 - acc: 0.5123 - val_loss: 0.5284 - val_acc: 0.5561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 401ms/step - loss: 0.5302 - acc: 0.5556 - val_loss: 0.5365 - val_acc: 0.5606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 406ms/step - loss: 0.5362 - acc: 0.5625 - val_loss: 0.5437 - val_acc: 0.5461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 401ms/step - loss: 0.5507 - acc: 0.5444 - val_loss: 0.5287 - val_acc: 0.5400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 405ms/step - loss: 0.5249 - acc: 0.5334 - val_loss: 0.4997 - val_acc: 0.5370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 399ms/step - loss: 0.5050 - acc: 0.5315 - val_loss: 0.4961 - val_acc: 0.5379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 402ms/step - loss: 0.4944 - acc: 0.5350 - val_loss: 0.4900 - val_acc: 0.5362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 401ms/step - loss: 0.4913 - acc: 0.5348 - val_loss: 0.4867 - val_acc: 0.5295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 398ms/step - loss: 0.4877 - acc: 0.5289 - val_loss: 0.4867 - val_acc: 0.5367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 407ms/step - loss: 0.4846 - acc: 0.5341 - val_loss: 0.4808 - val_acc: 0.5436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 425ms/step - loss: 0.4854 - acc: 0.5426 - val_loss: 0.5105 - val_acc: 0.5482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 404ms/step - loss: 0.5042 - acc: 0.5443 - val_loss: 0.5275 - val_acc: 0.5589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 400ms/step - loss: 0.5537 - acc: 0.5560 - val_loss: 0.6117 - val_acc: 0.4729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 402ms/step - loss: 0.5913 - acc: 0.4919 - val_loss: 0.4735 - val_acc: 0.5608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 415ms/step - loss: 0.4811 - acc: 0.5612 - val_loss: 0.4799 - val_acc: 0.5680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 402ms/step - loss: 0.4944 - acc: 0.5645 - val_loss: 0.5001 - val_acc: 0.5556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 401ms/step - loss: 0.4865 - acc: 0.5596 - val_loss: 0.4818 - val_acc: 0.5627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 401ms/step - loss: 0.4736 - acc: 0.5697 - val_loss: 0.4713 - val_acc: 0.5785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 405ms/step - loss: 0.4812 - acc: 0.5798 - val_loss: 0.4618 - val_acc: 0.5833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 401ms/step - loss: 0.4618 - acc: 0.5856 - val_loss: 0.4793 - val_acc: 0.5926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 408ms/step - loss: 0.4726 - acc: 0.5933 - val_loss: 0.4575 - val_acc: 0.5997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 409ms/step - loss: 0.4594 - acc: 0.5975 - val_loss: 0.4588 - val_acc: 0.6011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 403ms/step - loss: 0.4656 - acc: 0.5985 - val_loss: 0.4636 - val_acc: 0.5985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 418ms/step - loss: 0.4594 - acc: 0.6003 - val_loss: 0.4685 - val_acc: 0.5953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 402ms/step - loss: 0.4618 - acc: 0.6001 - val_loss: 0.4551 - val_acc: 0.5967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 401ms/step - loss: 0.4568 - acc: 0.5985 - val_loss: 0.4536 - val_acc: 0.5953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 407ms/step - loss: 0.4570 - acc: 0.5947 - val_loss: 0.4588 - val_acc: 0.5912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 399ms/step - loss: 0.4555 - acc: 0.5894 - val_loss: 0.4559 - val_acc: 0.5909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 402ms/step - loss: 0.4534 - acc: 0.5888 - val_loss: 0.4519 - val_acc: 0.5911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 403ms/step - loss: 0.4549 - acc: 0.5912 - val_loss: 0.4521 - val_acc: 0.5903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 419ms/step - loss: 0.4503 - acc: 0.5918 - val_loss: 0.4570 - val_acc: 0.5921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 409ms/step - loss: 0.4513 - acc: 0.5928 - val_loss: 0.4490 - val_acc: 0.5942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 406ms/step - loss: 0.4482 - acc: 0.5953 - val_loss: 0.4465 - val_acc: 0.5965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 410ms/step - loss: 0.4491 - acc: 0.5973 - val_loss: 0.4498 - val_acc: 0.5967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 400ms/step - loss: 0.4468 - acc: 0.5957 - val_loss: 0.4509 - val_acc: 0.5967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 406ms/step - loss: 0.4471 - acc: 0.5947 - val_loss: 0.4455 - val_acc: 0.5959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 399ms/step - loss: 0.4465 - acc: 0.5950 - val_loss: 0.4448 - val_acc: 0.5985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 401ms/step - loss: 0.4455 - acc: 0.5957 - val_loss: 0.4484 - val_acc: 0.5971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 407ms/step - loss: 0.4452 - acc: 0.5958 - val_loss: 0.4458 - val_acc: 0.5973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 403ms/step - loss: 0.4439 - acc: 0.5973 - val_loss: 0.4429 - val_acc: 0.5998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 405ms/step - loss: 0.4440 - acc: 0.5993 - val_loss: 0.4437 - val_acc: 0.5991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r10/10 [==============================] - 4s 404ms/step - loss: 0.4423 - acc: 0.5995 - val_loss: 0.4459 - val_acc: 0.6003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_model(pool_size,up_size, windows, seq_length, trainX, epochs, val_data):\n",
    "    encoder = Sequential(name=\"encoder\")\n",
    "    encoder.add(Conv1D(filters=300, kernel_size=n_features,\n",
    "               padding='same', name='Conv1'))\n",
    "    encoder.add(MaxPooling1D(pool_size=pool_size[0], name='Maxpool1'))\n",
    "    encoder.add(Conv1D(filters=200, kernel_size=n_features,\n",
    "               padding='same', name='Conv2'))\n",
    "    encoder.add(MaxPooling1D(pool_size=pool_size[1],  name=\"Maxpool2\"))\n",
    "    encoder.add(Conv1D(filters=150, kernel_size=n_features,\n",
    "               padding='same',name='Conv3'))\n",
    "    encoder.add(MaxPool1D(pool_size=pool_size[2], name=\"Maxpool3\"))\n",
    "    encoder.add(Conv1D(filters=100, kernel_size=n_features,\n",
    "               padding=\"same\", name='Conv4'))\n",
    "    encoder.add(MaxPool1D(pool_size=pool_size[3], name=\"Maxpool4\"))\n",
    "    encoder.add(Conv1D(filters=50, kernel_size=n_features,\n",
    "               padding=\"same\", name='Conv5'))\n",
    "    encoder.add(MaxPool1D(pool_size=pool_size[4], name=\"Maxpool5\"))\n",
    "    encoder.add(Conv1D(filters=latent_dim, kernel_size=n_features,          \n",
    "               padding=\"same\", name='Conv6'))\n",
    "    encoder.add(MaxPool1D(pool_size=pool_size[5], name=\"Maxpool6\"))\n",
    "    encoder.add(Flatten())\n",
    "    encoder.add(Dense(latent_dim, name='embedding'))\n",
    "    \n",
    "    decoder = Sequential(name=\"decoder\")\n",
    "    decoder.add(Dense(seq_length))\n",
    "    decoder.add(Reshape((windows, np.prod(up_size))))\n",
    "    decoder.add(Conv1D(filters=n_features, kernel_size=n_features,\n",
    "                padding='same', name='conv-decode1'))\n",
    "    decoder.add(UpSampling1D(size=up_size[0], name='upsampling1'))\n",
    "    decoder.add(Conv1D(filters=50, kernel_size=n_features,\n",
    "                padding='same', name='conv-decode2'))\n",
    "    decoder.add(UpSampling1D(size=up_size[1], name='upsampling2'))\n",
    "    decoder.add(Conv1D(filters=100, kernel_size=n_features,\n",
    "                padding='same', name='conv-decode3'))\n",
    "    decoder.add(UpSampling1D(size=up_size[2], name='upsampling3'))\n",
    "    decoder.add(Conv1D(filters=n_features, kernel_size=n_features,\n",
    "                padding='same', name='conv-decode4'))\n",
    "    decoder.add(UpSampling1D(size=up_size[3], name='upsampling4'))       \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(decoder)\n",
    "    \n",
    "    encoder.build(input_shape=(None,seq_length,n_features))\n",
    "    decoder.build(input_shape=(None,latent_dim))\n",
    "    model.build(input_shape=(None,seq_length, n_features))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['accuracy'])\n",
    "    encoder.summary()\n",
    "    decoder.summary()\n",
    "    model.fit(x=trainX, y=trainX, epochs=epochs, validation_data=(val_data, val_data))\n",
    "    model.save(\"Models/Anomalous_conv_compressor.h5\")\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "# model = create_model(pool_size=[2, 2, 2, 2, 2], windows=41,\n",
    "#                      up_size=[2, 2, 2, 2], seq_length=656, \n",
    "#                      trainX=trainX_nominal, epochs=15,\n",
    "#                       val_data=valX_nominal)\n",
    "model = create_model(pool_size=[2, 2, 2, 5, 5, 3], windows=33, \n",
    "                     up_size=[5, 5, 4, 2], seq_length=6600, \n",
    "                     trainX=trainX_anomalous, epochs=50,\n",
    "                     val_data=valX_anom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_conv_model(epochs, seq_length, trainX, valX_data):\n",
    "    model = Sequential()\n",
    "    \n",
    "    encoder = Sequential()\n",
    "    encoder.add(Conv1D(filters=200, input_shape=(seq_length, 8), padding='same', \n",
    "                       kernel_size=n_features))\n",
    "    encoder.add(Conv1D(filters=150, padding='same', \n",
    "                       kernel_size=n_features))\n",
    "    encoder.add(Conv1D(filters=100,padding='same', \n",
    "                       kernel_size=n_features))\n",
    "    encoder.add(Conv1D(filters=50, padding='same',\n",
    "                       kernel_size=n_features))\n",
    "    encoder.add(Conv1D(filters=latent_dim, padding='same',\n",
    "                       kernel_size=n_features)) \n",
    "    \n",
    "    decoder = Sequential(name=\"Decoder\")\n",
    "    decoder.add(Dense(64, name=\"Decoder_first\"))\n",
    "    decoder.add(Dense(units=n_features))\n",
    "    \n",
    "    model.add(encoder)\n",
    "    model.add(decoder)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x=trainX, y=trainX, epochs=epochs, validation_data=(valX_data,valX_data))\n",
    "    model.save(\"Models/Nominal_conv.h5\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# nom_model = create_conv_model(30, 656, trainX=trainX_nominal, \n",
    "#                             valX_data=valX_nominal)\n",
    "\n",
    "# anom_model = create_conv_model(50, 6600, trainX=trainX_anomalous, \n",
    "#                                valX_data=valX_anom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "nom_model = kr.models.load_model(\"Models/Nominal_conv.h5\")\n",
    "anom_model = kr.models.load_model(\"Models/Anomalous_conv.h5\") \n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 656, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6600, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nom_encoded = nom_model.get_layer(index=0).predict(trainX_nominal)\n",
    "print(nom_encoded.shape)\n",
    "# \n",
    "# plt.plot(nom_encoded[0])\n",
    "# plt.show()\n",
    "\n",
    "anom_encoded = anom_model.get_layer(index=0).predict(trainX_anomalous) \n",
    "print(anom_encoded.shape)\n",
    "# \n",
    "# plt.plot(anom_encoded)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(nom_encoded[0])\n",
    "plt.title(\"nom_encoding\")\n",
    "plt.show()\n",
    "plt.plot(anom_encoded[0])\n",
    "plt.title(\"anom_encoded\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (10,6600,10) into shape (10,10)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6acfad088c09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmod_enc_anom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmod_enc_anom\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0manom_encoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0manom_encoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manom_encoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m mod_enc_anom[0] = [-0.4481652 ,0.660993,3.6124344, 1.1903877,0.03680914,4.85686,\n\u001b[0;32m      5\u001b[0m                     -0.5965184, 3.8528519, -2.424729, 0.23854657]\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (10,6600,10) into shape (10,10)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "mod_enc_anom = np.zeros(shape=(10,10))\n",
    "\n",
    "mod_enc_anom[:anom_encoded.shape[0],:anom_encoded.shape[1]] = anom_encoded\n",
    "mod_enc_anom[0] = [-0.4481652 ,0.660993,3.6124344, 1.1903877,0.03680914,4.85686,\n",
    "                    -0.5965184, 3.8528519, -2.424729, 0.23854657]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected Decoder_input to have 3 dimensions, but got array with shape (10, 10)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-327df0b98ebe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmod_anom_dec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manom_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_enc_anom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod_anom_dec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manom_dec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"normal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected Decoder_input to have 3 dimensions, but got array with shape (10, 10)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "mod_anom_dec = anom_model.get_layer(index=1).predict(mod_enc_anom)\n",
    "print(mod_anom_dec.shape)\n",
    "plt.plot(anom_dec[0])\n",
    "plt.title(\"normal\")\n",
    "plt.show()\n",
    "plt.plot(mod_anom_dec[0])\n",
    "plt.title(\"modified\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_df = pd.DataFrame(mod_anom_dec[8], columns=boat_anom_csv.columns)\n",
    "plt.plot(mod_df[\"lon\"], mod_df['lat'])\n",
    "plt.title(\"reconstruction\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 656, 8)\n(10, 6600, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nom_dec = nom_model.get_layer(index=1).predict(nom_encoded)\n",
    "print(nom_dec.shape)\n",
    "\n",
    "anom_dec = anom_model.get_layer(index=1).predict(anom_encoded)\n",
    "print(anom_dec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(nom_dec[0], columns=boat_csv.columns)\n",
    "plt.plot(df[\"Lon\"], df['Lat'])\n",
    "plt.title(\"reconstruction_nom\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_anom = pd.DataFrame(anom_dec[0], columns=boat_anom_csv.columns)\n",
    "plt.plot(df_anom[\"lon\"], df_anom['lat'])\n",
    "plt.title(\"reconstruction_anom\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne(data, title):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    \n",
    "    tsne_obj= tsne.fit_transform(data)\n",
    "    tsne_df = pd.DataFrame({'X':tsne_obj[:,0],\n",
    "                            'Y':tsne_obj[:,1],\n",
    "                            })\n",
    "    \n",
    "    plt.scatter(x=tsne_df[\"X\"],y=tsne_df[\"Y\"], alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return tsne_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600\n"
     ]
    }
   ],
   "source": [
    "weight_modifiers = np.ones((latent_dim,))\n",
    "\n",
    "tsne_dec_nom_df = tsne(nom_dec[0], \"Decoded Nominal\")\n",
    "print(len(tsne_dec_nom_df))\n",
    "\n",
    "tsne_dec_anom_df = tsne(anom_dec[0], \"Decoded Anomalous\")\n",
    "print(len(tsne_dec_anom_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Customer Dendograms\")\n",
    "dend_nom = shc.dendrogram(shc.linkage(tsne_dec_nom_df, method='ward'))\n",
    "dend_anom = shc.dendrogram(shc.linkage(tsne_dec_anom_df, method='ward'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n_clusters = 4\n",
    "cluster = AgglomerativeClustering(n_clusters=n_clusters, \n",
    "                                  affinity='euclidean',\n",
    "                                  linkage='ward')\n",
    "cl_nom = cluster.fit_predict(tsne_dec_nom_df)\n",
    "cl_anom = cluster.fit_predict(tsne_dec_anom_df)\n",
    "plt.plot(cl_nom)\n",
    "plt.title(\"NOMINAL CLUSTERS\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(cl_anom)\n",
    "plt.title(\"Anomalous_clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_nom = pd.DataFrame(normal_data, columns=boat_csv.columns)\n",
    "df_anom = pd.DataFrame(anomalous_data, columns=boat_anom_csv.columns)\n",
    "def plot_clusters(cl, df, nominal):\n",
    "    for i in range(n_clusters):\n",
    "        cluster = np.squeeze(np.argwhere(cl==i))    \n",
    "        if nominal:\n",
    "            plt.scatter(x=df['Lon'][cluster],y=df[\"Lat\"][cluster],s=5)\n",
    "        else:\n",
    "            plt.scatter(x=df['lon'][cluster],y=df[\"lat\"][cluster],s=5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_clusters(cl_nom, df_nom, True)\n",
    "plot_clusters(cl_anom, df_anom, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_nominal = pd.DataFrame(normal_data, columns=boat_csv.columns)\n",
    "df = pd.DataFrame(nom_enc[0])\n",
    "\n",
    "plt.figure(1)\n",
    "axis_list = []\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    plt.plot(df[i])\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tsne_enc_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-e703b214c579>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mclusters_normal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans_normal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX_nominal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mkmeans_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtsne_enc_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mclusters_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtsne_enc_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tsne_enc_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "cluster_comp = [0,1,2,3]# print(tsne_enc_df)\n",
    "# print(tsne_dec_df)\n",
    "\n",
    "kmeans_normal = KMeans(n_clusters=2, random_state=0).fit(normal_data)\n",
    "clusters_normal = kmeans_normal.predict(np.average(trainX_nominal,axis=0))\n",
    "\n",
    "kmeans_enc = KMeans(n_clusters=2, random_state=0).fit(tsne_enc_df)\n",
    "clusters_enc = kmeans_enc.predict(tsne_enc_df)\n",
    "plt.plot(clusters_enc)\n",
    "plt.show()\n",
    "\n",
    "kmeans_dec = KMeans(n_clusters=2, random_state=0).fit(tsne_dec_df)\n",
    "clusters_dec = kmeans_dec.predict(tsne_dec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters_enc' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-3eff785d6bce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mmasks_normal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_mask_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters_normal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mmasks_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_mask_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mmasks_dec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_mask_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters_dec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clusters_enc' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def transform_to_mask(cl_label, clust_obj):\n",
    "    mask = []\n",
    "    part = []\n",
    "    for i, elem in enumerate(clust_obj):\n",
    "        if elem == cl_label:\n",
    "            part.append(i)\n",
    "        else:\n",
    "            if part:\n",
    "                mask.append(part)\n",
    "                part = []\n",
    "            else:\n",
    "                pass\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask_list(clust_obj):\n",
    "    mask_list = []\n",
    "    for i in cluster_comp:\n",
    "        mask_list.append(transform_to_mask(i, clust_obj))\n",
    "    return mask_list\n",
    "\n",
    "\n",
    "masks_normal = np.array(get_mask_list(clusters_normal))\n",
    "\n",
    "masks_enc = np.array(get_mask_list(clusters_enc))\n",
    "\n",
    "masks_dec = np.array(get_mask_list(clusters_dec))\n",
    "\n",
    "print(masks_normal)\n",
    "masks = (masks_normal, masks_enc,masks_dec)\n",
    "print(masks_normal.shape, masks_enc.shape, masks_enc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6620, 7)\n"
     ]
    }
   ],
   "source": [
    "def plot_cl(cl, color):\n",
    "    plt.plot(cl[\"lon\"], cl[\"lat\"], color=color)\n",
    "        \n",
    "    \n",
    "def plot_clusters_on_map():\n",
    "    titles = ['Nominal Normal','Nominal Encoded','Nominal Decoded']\n",
    "    color_list = ['blue','green','red','black']\n",
    "    map = anomalous_data[:6620]\n",
    "    print(map.shape)\n",
    "    for k,mask in enumerate(masks):\n",
    "        for i, elem in enumerate(mask):\n",
    "            for j in elem:\n",
    "                cl = pd.DataFrame(map[j], columns=boat_anom_csv.columns)\n",
    "                plot_cl(cl, color_list[i]) \n",
    "        plt.title(titles[k])\n",
    "        plt.show()\n",
    "      \n",
    "               \n",
    "plot_clusters_on_map()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
