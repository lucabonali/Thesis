{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66256, 7)\n(6620, 7)\n(100, 656, 7)\n(10, 656, 7)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras import Model\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Sequential\n",
    "import keras as kr\n",
    "import keras.losses as losses\n",
    " \n",
    " \n",
    "# Setup the network parameters:\n",
    "intermediate_dim = 300\n",
    "latent_dim = 10\n",
    "beta = 5\n",
    "epochs = 15\n",
    "n_sequences = 101\n",
    "n_features = 7\n",
    "\n",
    "boat_csv = pd.read_csv(\"Data/Boat_nominal_data/Boat_sequences.csv\")\n",
    "boat_csv = boat_csv.drop(columns=[\"Unnamed: 0\"])\n",
    "# boat_csv = boat_csv[:5739]\n",
    "scaler = StandardScaler()\n",
    "normal_data = scaler.fit_transform(boat_csv)\n",
    "print(normal_data.shape)\n",
    "\n",
    "boat_curved = pd.read_csv(\"Data/Anomalous_boat_data.csv\")\n",
    "boat_curved= boat_curved.drop(columns=[\"Unnamed: 0\", \"heading\"])    \n",
    "scaler = StandardScaler()\n",
    "anomalous_data = scaler.fit_transform(boat_curved)\n",
    "print(anomalous_data.shape)\n",
    "\n",
    "batch_size =  656\n",
    "interval = 656\n",
    "def prepare_sequences(data):\n",
    "    samples = []\n",
    "    for i in range(0,data.shape[0]- batch_size, interval):\n",
    "        sample = data[i:i+batch_size]\t\n",
    "        samples.append(sample)\n",
    "\n",
    "    sequences = np.array(samples)\n",
    "\n",
    "    # Batch size (Number of samples time steps and number of features\n",
    "    trainX = np.reshape(sequences, (len(sequences), batch_size, 7))\n",
    "\n",
    "    return trainX\n",
    "\n",
    "trainX_nominal = prepare_sequences(normal_data) \n",
    "print(trainX_nominal.shape)\n",
    "\n",
    "input_length = trainX_nominal.shape[0]\n",
    "trainX_anomalous = prepare_sequences(anomalous_data)\n",
    "print(trainX_anomalous.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected sequential_2_input to have 3 dimensions, but got array with shape (6620, 7)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-671f8f88a31b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mse\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mboat_curved\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mboat_curved\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected sequential_2_input to have 3 dimensions, but got array with shape (6620, 7)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# n_features = 7\n",
    "# pool_size = 2\n",
    "# x = Input(shape=(656, 7), name=\"input\")\n",
    "# h = x\n",
    "# h = Conv1D(filters=300, kernel_size=n_features,\n",
    "#            padding='same', name='Conv1')(h)\n",
    "# h = MaxPooling1D(pool_size=pool_size, name='Maxpool1')(h)\n",
    "# h = Conv1D(filters=200, kernel_size=n_features,\n",
    "#            padding='same', name='Conv2')(h)\n",
    "# h = MaxPooling1D(pool_size=pool_size,  name=\"Maxpool2\")(h)\n",
    "# h = Conv1D(filters=150, kernel_size=n_features,\n",
    "#            name='Conv3')(h)\n",
    "# h = MaxPool1D(pool_size=pool_size, name=\"Maxpool3\")(h)\n",
    "# h = Conv1D(filters=100, kernel_size=n_features,\n",
    "#            padding=\"same\", name='Conv4')(h)\n",
    "# h = MaxPool1D(pool_size=pool_size, name=\"Maxpool4\")(h)\n",
    "# h = Conv1D(filters=latent_dim, kernel_size=n_features,\n",
    "#            padding=\"same\", name='Conv5')(h)\n",
    "# h = MaxPool1D(pool_size=pool_size, name=\"Maxpool5\")(h)\n",
    "# \n",
    "# h = Flatten()(h)\n",
    "# \n",
    "# h = Dense(latent_dim, name='embedding') (h)\n",
    "# y = h\n",
    "# \n",
    "# y = Dense(656, activation=\"relu\") (y)\n",
    "# y = Reshape((41, 16))(y)\n",
    "# y = Conv1D(filters=7, kernel_size=n_features,\n",
    "#            padding='same', name='conv-decode1')(y)\n",
    "# y = UpSampling1D(size=2, name='upsampling1')(y)\n",
    "# y = Conv1D(filters=150, kernel_size=n_features,\n",
    "#            padding='same', name='conv-decode2')(y)\n",
    "# y = UpSampling1D(size=2, name='upsampling2')(y)\n",
    "# y = Conv1D(filters=200, kernel_size=n_features,\n",
    "#            padding='same', name='conv-decode3')(y)\n",
    "# y = UpSampling1D(size=2, name='upsampling3')(y)\n",
    "# y = Conv1D(filters=7, kernel_size=n_features,\n",
    "#            padding='same', name='conv-decode4')(y)\n",
    "# y = UpSampling1D(size=2, name='upsampling4')(y)\n",
    "# # y = Conv1D(filters=7, kernel_size=n_features,\n",
    "# #            padding='same', name='conv-decode5')(y)\n",
    "# # y = UpSampling1D(size=2, name='upsampling5')(y)\n",
    "# \n",
    "# \n",
    "# model = Model(inputs=x, outputs=y, name='AE')\n",
    "# enc = Model(inputs=x, outputs=h, name='encoder')\n",
    "# \n",
    "# model.summary()\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "encoder = Sequential()\n",
    "encoder.add(Conv1D(filters=200, input_shape=(None,7), padding='same', \n",
    "                   kernel_size=n_features))\n",
    "encoder.add(Conv1D(filters=150, padding='same', \n",
    "                   kernel_size=n_features))\n",
    "encoder.add(Conv1D(filters=100,padding='same', \n",
    "                   kernel_size=n_features))\n",
    "encoder.add(Conv1D(filters=50, padding='same',\n",
    "                   kernel_size=n_features))\n",
    "encoder.add(Conv1D(filters=latent_dim, padding='same',\n",
    "                   kernel_size=n_features))\n",
    "\n",
    "decoder = Sequential()\n",
    "decoder.add(Dense(units=n_features))\n",
    "\n",
    "model.add(encoder)\n",
    "model.add(decoder)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=boat_curved, y=boat_curved, epochs=epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne(data, title):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    \n",
    "    tsne_obj= tsne.fit_transform(data)\n",
    "    tsne_df = pd.DataFrame({'X':tsne_obj[:,0],\n",
    "                            'Y':tsne_obj[:,1],\n",
    "                            })\n",
    "    \n",
    "    plt.scatter(x=tsne_df[\"X\"],y=tsne_df[\"Y\"], alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return tsne_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1000 into shape (100,40)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-711a0f39ec0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnom_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnom_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnom_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# weight_modifiers[0] = 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1000 into shape (100,40)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "weight_modifiers = np.ones((latent_dim,))\n",
    "\n",
    "nom_enc =(encoder.predict(trainX_nominal))\n",
    "print(nom_enc.shape)\n",
    "\n",
    "nom_enc = nom_enc.reshape(100,latent_dim)\n",
    "\n",
    "# weight_modifiers[0] = 1\n",
    "# nom_enc_modified = nom_enc\n",
    "# for i, elem in enumerate(nom_enc_modified):\n",
    "#     nom_enc_modified[i] = elem * weight_modifiers    \n",
    "for i,elem in enumerate(nom_enc):\n",
    "    plt.scatter(y=nom_enc[i], x=np.linspace(1,latent_dim,latent_dim))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# tsne_enc_df = tsne(np.average(nom_enc, axis=0), \"Encoded Nominal\")\n",
    "# \n",
    "# \n",
    "# nom_dec = decoder.predict(nom_enc)\n",
    "# tsne_dec_df = tsne(np.average(nom_dec, axis=0), \"Decoded Nominal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_nominal = pd.DataFrame(normal_data, columns=boat_csv.columns)\n",
    "df = pd.DataFrame(np.average(nom_enc, axis=0))\n",
    "\n",
    "plt.figure(1)\n",
    "axis_list = []\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    plt.plot(df[i])\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "cluster_comp = [0,1,2,3]# print(tsne_enc_df)\n",
    "# print(tsne_dec_df)\n",
    "\n",
    "kmeans_normal = KMeans(n_clusters=4, random_state=0).fit(normal_data)\n",
    "clusters_normal = kmeans_normal.predict(np.average(trainX_nominal,axis=0))\n",
    "\n",
    "kmeans_enc = KMeans(n_clusters=4, random_state=0).fit(tsne_enc_df)\n",
    "clusters_enc = kmeans_enc.predict(tsne_enc_df)\n",
    "\n",
    "kmeans_dec = KMeans(n_clusters=4, random_state=0).fit(tsne_dec_df)\n",
    "clusters_dec = kmeans_dec.predict(tsne_dec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) (4,) (4,)\n"
     ]
    }
   ],
   "source": [
    "def transform_to_mask(cl_label, clust_obj):\n",
    "    mask = []\n",
    "    part = []\n",
    "    for i, elem in enumerate(clust_obj):\n",
    "        if elem == cl_label:\n",
    "            part.append(i)\n",
    "        else:\n",
    "            if part:\n",
    "                mask.append(part)\n",
    "                part = []\n",
    "            else:\n",
    "                pass\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask_list(clust_obj):\n",
    "    mask_list = []\n",
    "    for i in cluster_comp:\n",
    "        mask_list.append(transform_to_mask(i, clust_obj))\n",
    "    return mask_list\n",
    "\n",
    "\n",
    "masks_normal = np.array(get_mask_list(clusters_normal))\n",
    "\n",
    "masks_enc = np.array(get_mask_list(clusters_enc))\n",
    "\n",
    "masks_dec = np.array(get_mask_list(clusters_dec))\n",
    "\n",
    "masks = (masks_normal, masks_enc,masks_dec)\n",
    "print(masks_normal.shape, masks_enc.shape, masks_enc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66963, 7)\n"
     ]
    }
   ],
   "source": [
    "def plot_cl(cl, color):\n",
    "    plt.plot(cl[\"Lon\"], cl[\"Lat\"], color=color)\n",
    "        \n",
    "    \n",
    "def plot_clusters_on_map():\n",
    "    titles = ['Nominal Normal','Nominal Encoded','Nominal Decoded']\n",
    "    color_list = ['blue','green','red','black']\n",
    "    map = normal_data\n",
    "    print(map.shape)\n",
    "    for k,mask in enumerate(masks):\n",
    "        for i, elem in enumerate(mask):\n",
    "            for j in elem:\n",
    "                cl = pd.DataFrame(map[j], columns=boat_csv.columns)\n",
    "                plot_cl(cl, color_list[i]) \n",
    "        plt.title(titles[k])\n",
    "        plt.show()\n",
    "      \n",
    "               \n",
    "plot_clusters_on_map()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
