{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307200, 4)\n(300, 1024, 4)\n(30, 1024, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Conv1D,UpSampling1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "n_features = 4\n",
    "boat_csv = pd.read_csv(\"Data/Boat_nominal_data/Boat_sequences_mix.csv\")\n",
    "boat_csv = boat_csv.drop(columns=[\"Unnamed: 0\", \"M0C\", \"M1C\", \"Acceleration\",\"Speed\"])\n",
    "scaler = StandardScaler()\n",
    "normal_data = scaler.fit_transform(boat_csv)\n",
    "print(normal_data.shape)\n",
    "\n",
    "boat_val = pd.read_csv(\"Data/Boat_nominal_data/Boat_sequence_mix_val.csv\")\n",
    "boat_val = boat_val.drop(columns=[\"Unnamed: 0\", \"M0C\", \"M1C\", \"Acceleration\",\"Speed\"])\n",
    "scaler = StandardScaler()\n",
    "val_nom_data = scaler.fit_transform(boat_val)\n",
    "\n",
    "def prepare_sequences(data, batch_size):\n",
    "    samples = []\n",
    "    for i in range(0,data.shape[0], batch_size):\n",
    "        sample = data[i:i+batch_size]\t\n",
    "        samples.append(sample)\n",
    "    sequences = np.array(samples)\n",
    "    trainX = np.reshape(sequences, (len(sequences), batch_size, n_features))\n",
    "    return trainX\n",
    "\n",
    "\n",
    "def prepare_data():    \n",
    "    trainX_nominal = prepare_sequences(normal_data,1024) \n",
    "    print(trainX_nominal.shape)\n",
    "    \n",
    "    valX_nominal = prepare_sequences(val_nom_data,1024)\n",
    "    print(valX_nominal.shape)\n",
    "\n",
    "    return trainX_nominal, valX_nominal\n",
    "\n",
    "trainX_nominal, valX_nominal = prepare_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import MaxPooling1D, RepeatVector, LSTM\n",
    "from keras_preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "input_shape = (1024, n_features)\n",
    "latent_dim = 20\n",
    "use_mse = True   \n",
    "load_weights = False\n",
    "\n",
    "def create_vae():\n",
    "    units = len(trainX_nominal)\n",
    "\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    x = inputs\n",
    "    for i in range(2):\n",
    "        x = Conv1D(units=units,activation='relu')(x)\n",
    "    #lstm, state_h, state_c = LSTM(latent_dim,return_state=True)(x)\n",
    "    embeddings = Dense(latent_dim, name=\"embeddings\", activation='relu')(x)\n",
    "\n",
    "    encoder = Model(inputs, embeddings, name='encoder')\n",
    "    encoder.summary()\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = Dense(100)(latent_inputs)\n",
    "    #x = RepeatVector(1024)(x)\n",
    "    #x = LSTM(latent_dim, return_sequences=True)(x)\n",
    "    #outputs = LSTM(4, return_sequences=True)(x)\n",
    "    x = RepeatVector(1024)(x)\n",
    "    for i in range(2):\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "    #x = RepeatVector(1024)(x)\n",
    "    outputs = Dense(4)(x)\n",
    "    decoder = Model(latent_inputs, outputs)\n",
    "    decoder.summary()\n",
    "    vae = Model(inputs, outputs)\n",
    "    #vae.summary()\n",
    "    #vae.add(decoder)\n",
    "    vae.compile(optimizer='rmsprop', loss='mse', metrics= ['accuracy'])\n",
    "    return (vae, encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'filters' and 'kernel_size'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-526cf0683b60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_vae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m checkpointer = ModelCheckpoint(filepath=\"Models/Weights/Nominal_weights_ae.hdf5\",\n\u001b[0;32m      5\u001b[0m                                verbose=1, save_best_only=True)\n",
      "\u001b[1;32m<ipython-input-133-221413ae36d9>\u001b[0m in \u001b[0;36mcreate_vae\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;31m#lstm, state_h, state_c = LSTM(latent_dim,return_state=True)(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"embeddings\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'filters' and 'kernel_size'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "vae, encoder, decoder = create_vae()\n",
    "checkpointer = ModelCheckpoint(filepath=\"Models/Weights/Nominal_weights_ae.hdf5\",\n",
    "                               verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 30 samples\nEpoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r300/300 [==============================] - 30s 99ms/step - loss: 1.0028 - acc: 0.3105 - val_loss: 1.0010 - val_acc: 0.2331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00001: val_loss improved from inf to 1.00097, saving model to Models/Weights/Nominal_weights_ae.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r300/300 [==============================] - 12s 39ms/step - loss: 1.0010 - acc: 0.2328 - val_loss: 1.0086 - val_acc: 0.2163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00002: val_loss did not improve from 1.00097\nEpoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r300/300 [==============================] - 12s 39ms/step - loss: 1.0086 - acc: 0.2206 - val_loss: 1.0072 - val_acc: 0.2426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00003: val_loss did not improve from 1.00097\nEpoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r300/300 [==============================] - 11s 38ms/step - loss: 1.0072 - acc: 0.2361 - val_loss: 1.0002 - val_acc: 0.2163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00004: val_loss improved from 1.00097 to 1.00024, saving model to Models/Weights/Nominal_weights_ae.hdf5\nEpoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r300/300 [==============================] - 12s 40ms/step - loss: 1.0002 - acc: 0.2207 - val_loss: 1.0007 - val_acc: 0.2350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00005: val_loss did not improve from 1.00024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vae.fit(x=trainX_nominal,\n",
    "        y=trainX_nominal,\n",
    "        epochs=5,\n",
    "        validation_data=(valX_nominal,valX_nominal),\n",
    "        batch_size=1024,\n",
    "        callbacks=[checkpointer])\n",
    "vae.load_weights('Models/Weights/Nominal_weights_ae.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 20)\n"
     ]
    }
   ],
   "source": [
    "vaes = []\n",
    "vaes.append(vae)\n",
    "\n",
    "def return_mask(num, labels):\n",
    "    return np.squeeze(np.argwhere(labels == num))\n",
    "\n",
    "labels = pd.read_csv(\"Data/Boat_nominal_data/Boat_mix_labels.csv\")\n",
    "labels = labels.drop(columns=\"Unnamed: 0\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "masks = [return_mask(num,labels)[:,0] for num in range(0,9)]\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model_index = 0\n",
    "# \n",
    "# for i in vaes:\n",
    "#     encodings.append(i[1].predict(trainX_nominal))\n",
    "\n",
    "encodings = []\n",
    "#encodings.append(np.average(encoder.predict(trainX_nominal), axis=1))\n",
    "encodings.append(encoder.predict(trainX_nominal))\n",
    "print(encodings[0].shape)\n",
    "\n",
    "def check_z_sampling(encoded_values):\n",
    "    m = encoded_values[0]\n",
    "    var = np.exp(0.5*encoded_values[1])\n",
    "    eps = np.random.normal(0,1,latent_dim)\n",
    "    \n",
    "    sampled = []\n",
    "    index = 0\n",
    "    var_zero = np.zeros(10)\n",
    "    for means in m:\n",
    "        sample = means+var[index]*eps\n",
    "        #sample = means+var_zero*eps \n",
    "        sampled.append(sample)\n",
    "        index += 1\n",
    "    \n",
    "    sampled = np.array(sampled)\n",
    "    return sampled\n",
    "\n",
    "def plot_pca(title, type): \n",
    "    x_val = []\n",
    "    y_val= []\n",
    "    for i in range(principalComponents.shape[0]):\n",
    "        x_val.append(principalComponents[i][0])\n",
    "        y_val.append(principalComponents[i][1])\n",
    "    x_val = np.array(x_val)\n",
    "    y_val = np.array(y_val)\n",
    "    \n",
    "    for mask in masks:\n",
    "        plt.scatter(x=x_val[mask], y=y_val[mask], alpha=0.5)\n",
    "\n",
    "    plt.legend(labels=np.arange(0,9))\n",
    "    plt.title(str(title)+\"\"+type)\n",
    "    plt.show()\n",
    "    \n",
    "#print(encodings[0][0][0], encodings[0][1][0], encodings[0][2][0])\n",
    "for i, encod in enumerate(encodings):\n",
    "        latent_values = check_z_sampling(encod)\n",
    "        scaler = StandardScaler()\n",
    "        enc_input = scaler.fit_transform(encod) \n",
    "        pca = PCA(2)\n",
    "        principalComponents = pca.fit_transform(enc_input)\n",
    "        #print(pca.explained_variance_ratio_)\n",
    "        plot_pca('?', '?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 1024, 4)\n(15, 1024, 4)\n(11, 1024, 4)\n(12, 1024, 4)\n(14, 1024, 4)\n(11, 1024, 4)\n(13, 1024, 4)\n(16, 1024, 4)\n(16, 1024, 4)\n"
     ]
    }
   ],
   "source": [
    "runs = []\n",
    "for mask in masks:\n",
    "    run_for_class = trainX_nominal[mask]\n",
    "    print(run_for_class.shape)\n",
    "    runs.append(run_for_class)\n",
    "    \n",
    "for i in runs[3]:\n",
    "    run = np.reshape(i, (1, 1024,4))\n",
    "    rec = vaes[0].predict(run)\n",
    "    rec = np.reshape(rec, (len(trainX_nominal[0]), n_features))\n",
    "    reconstruction_df = pd.DataFrame(rec, columns=boat_csv.columns)\n",
    "    plt.plot(reconstruction_df[\"Lon\"], reconstruction_df[\"Lat\"])\n",
    "    #plt.savefig(\"Imgs/Latent_reconstruction/\"+str(title)+\".png\")\n",
    "    plt.show()\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encodings = encoder.predict(trainX_nominal)\n",
    "\n",
    "\n",
    "labels = pd.read_csv(\"Data/Boat_nominal_data/Boat_mix_labels.csv\")\n",
    "labels = labels.drop(columns=\"Unnamed: 0\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "mat_mask = np.array([labels for i in range(latent_dim)])\n",
    "print(mat_mask.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "labels = np.array(pd.read_csv(\"Data/Boat_nominal_data/Boat_mix_labels.csv\")['label']) \n",
    "nominals = np.squeeze(np.argwhere(labels==1))\n",
    "anomalous = np.squeeze(np.argwhere(labels==0))\n",
    "print(type(nominals), type(nominals[0]))\n",
    "\n",
    "model_index = 0\n",
    "\n",
    "titles = [\"Mean\", \"Std\", \"Sampled\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "enc_input = scaler.fit_transform(encodings)\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(enc_input)\n",
    "x_val = []\n",
    "y_val=[]\n",
    "for i in range(principalComponents.shape[0]):\n",
    "    x_val.append(principalComponents[i][0])\n",
    "    y_val.append(principalComponents[i][1])\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "\n",
    "plt.scatter(x=x_val[nominals],y=y_val[nominals], alpha=0.5)\n",
    "plt.scatter(x=x_val[anomalous],y=y_val[anomalous], alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne(data, title):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    \n",
    "    tsne_obj= tsne.fit_transform(data)\n",
    "    tsne_df = pd.DataFrame({'X':tsne_obj[:,0],\n",
    "                            'Y':tsne_obj[:,1],\n",
    "                            })\n",
    "    \n",
    "    plt.scatter(x=tsne_df[\"X\"][nominals],\n",
    "                y=tsne_df[\"Y\"][nominals], alpha=0.5)\n",
    "    plt.scatter(x=tsne_df[\"X\"][anomalous],\n",
    "                y=tsne_df[\"Y\"][anomalous], alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return tsne_df\n",
    "\n",
    "tsne_enc_nom_df = tsne(encodings, \"Values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_latent_space(xCoord, yCoord):\n",
    "    point = np.array([xCoord,yCoord])\n",
    "    latent_point = pca.inverse_transform(point)\n",
    "    return np.reshape(latent_point,(1, latent_dim))\n",
    "\n",
    "def visualize_reconstruction(reconstructed_run, title):\n",
    "    plt.plot(reconstructed_run[\"Lon\"], reconstructed_run[\"Lat\"])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    # plt.plot(reconstructed_run[\"Sin\"])\n",
    "    # plt.plot(reconstructed_run[\"Cosin\"])\n",
    "    # plt.show()\n",
    "\n",
    "X,Y = np.mgrid[-5:5.1:0.5, -5:5.1:0.5]\n",
    "XY = np.vstack((X.flatten(), Y.flatten())).T\n",
    "\n",
    "point = [0,0]\n",
    "\n",
    "for i in XY:\n",
    "    point = sample_from_latent_space(i[0],i[1])\n",
    "    reconstructed = np.reshape(decoder.predict(point), (len(trainX_nominal[0]),n_features))\n",
    "    visualize_reconstruction(pd.DataFrame(reconstructed, columns=boat_csv.columns), \n",
    "                         title=\"Reconstruction\")\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {
    "collapsed": true
   },
   "level": 1,
   "source": [
    "OLD STUFF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Customer Dendograms\")\n",
    "dend_nom = shc.dendrogram(shc.linkage(tsne_dec_nom_df, method='ward'))\n",
    "dend_anom = shc.dendrogram(shc.linkage(tsne_dec_anom_df, method='ward'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n_clusters = 4\n",
    "cluster = AgglomerativeClustering(n_clusters=n_clusters, \n",
    "                                  affinity='euclidean',\n",
    "                                  linkage='ward')\n",
    "cl_nom = cluster.fit_predict(tsne_dec_nom_df)\n",
    "cl_anom = cluster.fit_predict(tsne_dec_anom_df)\n",
    "plt.plot(cl_nom)\n",
    "plt.title(\"NOMINAL CLUSTERS\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(cl_anom)\n",
    "plt.title(\"Anomalous_clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_nom = pd.DataFrame(normal_data, columns=boat_csv.columns)\n",
    "df_anom = pd.DataFrame(anomalous_data, columns=boat_anom_csv.columns)\n",
    "def plot_clusters(cl, df, nominal):\n",
    "    for i in range(n_clusters):\n",
    "        cluster = np.squeeze(np.argwhere(cl==i))    \n",
    "        if nominal:\n",
    "            plt.scatter(x=df['Lon'][cluster],y=df[\"Lat\"][cluster],s=5)\n",
    "        else:\n",
    "            plt.scatter(x=df['lon'][cluster],y=df[\"lat\"][cluster],s=5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_clusters(cl_nom, df_nom, True)\n",
    "plot_clusters(cl_anom, df_anom, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_nominal = pd.DataFrame(normal_data, columns=boat_csv.columns)\n",
    "df = pd.DataFrame(nom_enc[0])\n",
    "\n",
    "plt.figure(1)\n",
    "axis_list = []\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    plt.plot(df[i])\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tsne_enc_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-e703b214c579>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mclusters_normal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans_normal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX_nominal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mkmeans_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtsne_enc_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mclusters_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtsne_enc_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tsne_enc_df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "cluster_comp = [0,1,2,3]# print(tsne_enc_df)\n",
    "# print(tsne_dec_df)\n",
    "\n",
    "kmeans_normal = KMeans(n_clusters=2, random_state=0).fit(normal_data)\n",
    "clusters_normal = kmeans_normal.predict(np.average(trainX_nominal,axis=0))\n",
    "\n",
    "kmeans_enc = KMeans(n_clusters=2, random_state=0).fit(tsne_enc_df)\n",
    "clusters_enc = kmeans_enc.predict(tsne_enc_df)\n",
    "plt.plot(clusters_enc)\n",
    "plt.show()\n",
    "\n",
    "kmeans_dec = KMeans(n_clusters=2, random_state=0).fit(tsne_dec_df)\n",
    "clusters_dec = kmeans_dec.predict(tsne_dec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters_enc' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-3eff785d6bce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mmasks_normal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_mask_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters_normal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mmasks_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_mask_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mmasks_dec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_mask_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters_dec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clusters_enc' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def transform_to_mask(cl_label, clust_obj):\n",
    "    mask = []\n",
    "    part = []\n",
    "    for i, elem in enumerate(clust_obj):\n",
    "        if elem == cl_label:\n",
    "            part.append(i)\n",
    "        else:\n",
    "            if part:\n",
    "                mask.append(part)\n",
    "                part = []\n",
    "            else:\n",
    "                pass\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_mask_list(clust_obj):\n",
    "    mask_list = []\n",
    "    for i in cluster_comp:\n",
    "        mask_list.append(transform_to_mask(i, clust_obj))\n",
    "    return mask_list\n",
    "\n",
    "\n",
    "masks_normal = np.array(get_mask_list(clusters_normal))\n",
    "\n",
    "masks_enc = np.array(get_mask_list(clusters_enc))\n",
    "\n",
    "masks_dec = np.array(get_mask_list(clusters_dec))\n",
    "\n",
    "print(masks_normal)\n",
    "masks = (masks_normal, masks_enc,masks_dec)\n",
    "print(masks_normal.shape, masks_enc.shape, masks_enc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6620, 7)\n"
     ]
    }
   ],
   "source": [
    "def plot_cl(cl, color):\n",
    "    plt.plot(cl[\"lon\"], cl[\"lat\"], color=color)\n",
    "        \n",
    "    \n",
    "def plot_clusters_on_map():\n",
    "    titles = ['Nominal Normal','Nominal Encoded','Nominal Decoded']\n",
    "    color_list = ['blue','green','red','black']\n",
    "    map = anomalous_data[:6620]\n",
    "    print(map.shape)\n",
    "    for k,mask in enumerate(masks):\n",
    "        for i, elem in enumerate(mask):\n",
    "            for j in elem:\n",
    "                cl = pd.DataFrame(map[j], columns=boat_anom_csv.columns)\n",
    "                plot_cl(cl, color_list[i]) \n",
    "        plt.title(titles[k])\n",
    "        plt.show()\n",
    "      \n",
    "               \n",
    "plot_clusters_on_map()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
