{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Conv1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "n_features = 4 \n",
    "labels = pd.read_csv(\"Data/Boat_nominal_data/Boat_mix_len_labels.csv\")\n",
    "labels = labels.drop(columns=\"Unnamed: 0\") \n",
    "labels = np.array(labels)\n",
    "max_len = 0\n",
    "\n",
    "\n",
    "def prepare_training(path, n_runs):\n",
    "    def get_max_len(sequence_list):\n",
    "        m_len = 0\n",
    "        for seq in sequence_list:\n",
    "            if len(seq) > m_len:\n",
    "                m_len = len(seq)\n",
    "        max_len = m_len\n",
    "        print(max_len)\n",
    "        return m_len\n",
    "    \n",
    "    \n",
    "    def construct_matrix(sequence_list):\n",
    "        max_len = get_max_len(sequence_list)\n",
    "        train_matrix = np.zeros(shape=(n_runs, max_len, n_features))\n",
    "        for index,run in enumerate(sequence_list):\n",
    "            train_matrix[index, :len(run)] = run\n",
    "        print(train_matrix.shape)\n",
    "        return train_matrix\n",
    "        \n",
    "        \n",
    "    def stadard_sequences(sequences):\n",
    "        for i, seq in enumerate(sequences):\n",
    "            sequences[i] = MinMaxScaler(feature_range=[0, 1]).fit_transform(seq)\n",
    "        return sequences       \n",
    "    \n",
    "    def read_sequences():\n",
    "        run_list_mix = []\n",
    "        for index in range(n_runs):\n",
    "            run_csv = pd.read_csv(path+str(index))\n",
    "            run_csv = run_csv.drop(columns=['Unnamed: 0'])\n",
    "            run_list_mix.append(run_csv)\n",
    "        stand_sequences = stadard_sequences(run_list_mix)\n",
    "        padded_matrix = construct_matrix(stand_sequences)\n",
    "        return padded_matrix\n",
    "    \n",
    "    return read_sequences()\n",
    "    \n",
    "\n",
    "train_matrix = prepare_training(\"Mix_sequences_var_length/run^\", n_runs=400) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 900, 4)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 450, 50)           450       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 225, 50)           5050      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 11250)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                112510    \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 900, 4)            134054    \n",
      "=================================================================\n",
      "Total params: 252,064\n",
      "Trainable params: 252,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Input, LSTM, RepeatVector, Conv2DTranspose\n",
    "from keras.losses import mse\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "filters = 50\n",
    "intermediate_dimension = 30 \n",
    "latent_dim = 10\n",
    "\n",
    "def Conv1DTranspose(input_tensor, filters, kernel_size,last, strides=2, padding='same'):\n",
    "        if last:\n",
    "            activation = 'linear'\n",
    "        else:\n",
    "            activation = 'relu'\n",
    "        x = Lambda(lambda x: K.expand_dims(x, axis=2))(input_tensor)\n",
    "        x = Conv2DTranspose(filters=filters, kernel_size=(kernel_size, 1), \n",
    "                            strides=(strides, 1), padding=padding,\n",
    "                            activation=activation)(x)\n",
    "        x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def repeat(x):\n",
    "    steps_matrix = K.ones_like(x[0][:, :, :1])\n",
    "    latent_matrix = K.expand_dims(x[1], axis=1)\n",
    "    return K.batch_dot(steps_matrix, latent_matrix)\n",
    "\n",
    "\n",
    "def create_vae():\n",
    "    print(max_len)\n",
    "    inputs = Input(shape=(180, n_features))\n",
    "    x = inputs\n",
    "    \n",
    "    for i in range(2):\n",
    "        x = Conv1D(filters=filters,\n",
    "                   kernel_size=2,\n",
    "                   strides=2,\n",
    "                   padding='same')(x)\n",
    "    \n",
    "    shape = K.int_shape(x)\n",
    "    \n",
    "    #before_flattening = LSTM(intermediate_dimension, return_sequences=True)(x)\n",
    "    #x_flat = LSTM(intermediate_dimension)(before_flattening)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    \n",
    "    embeddings = Dense(latent_dim)(x)\n",
    "    \n",
    "    z_mean = Dense(latent_dim, name='z_mean',)(embeddings)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(embeddings)\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    latent_inputs = Input(shape=(latent_dim,), name='latent_inputs')\n",
    "    #x = Lambda(repeat)([before_flattening, z])\n",
    "    x = Dense(shape[1]*shape[2])(latent_inputs)\n",
    "    x = Reshape((shape[1],shape[2]))(x)\n",
    "    \n",
    "    \n",
    "    print(K.int_shape(x))\n",
    "    for i in range(2):\n",
    "        x = Conv1DTranspose(input_tensor=x,\n",
    "                            filters=filters,\n",
    "                            kernel_size=2,\n",
    "                            last=False,\n",
    "                            padding='same')\n",
    "    \n",
    "    \n",
    "    outputs = Conv1DTranspose(input_tensor=x,\n",
    "                              filters=n_features,\n",
    "                              kernel_size=2,\n",
    "                              strides=1,\n",
    "                              last=True,\n",
    "                              padding='same')\n",
    "    \n",
    "    encoder.summary()\n",
    "    decoder = Model(latent_inputs, outputs)\n",
    "    decoder.summary()\n",
    "    outputs = decoder(encoder.outputs[2])\n",
    "    reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "    #reconstruction_loss *= sequence_length*4\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), \n",
    "                             axis=-1)\n",
    "    loss = K.mean(reconstruction_loss+0*kl_loss)\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "    vae.add_loss(loss)\n",
    "    #vae.summary()\n",
    "    vae.compile(optimizer='adam')\n",
    "    return vae, encoder\n",
    "\n",
    "\n",
    "def create_ae():\n",
    "    inputs = Input(shape=(900, n_features))\n",
    "    x = inputs\n",
    "    for i in range(2):\n",
    "        x = Conv1D(filters=filters, kernel_size=2, strides=2,\n",
    "                   activation='relu', padding='same')(x)\n",
    "    shape = K.int_shape(x)   \n",
    "    before_flattening = x #LSTM(filters, return_sequences=True)(x)   \n",
    "    #encoded = LSTM(intermediate_dimension)(before_flattening)\n",
    "    encoded = Flatten()(x)\n",
    "    encoded = Dense(latent_dim)(encoded)\n",
    "    #x = Lambda(repeat)([before_flattening, encoded])\n",
    "    \n",
    "    latent_inputs = Input(shape=(latent_dim,), name='latent_inputs')\n",
    "    x = Dense(shape[1]*shape[2])(latent_inputs)\n",
    "    x = Reshape((shape[1],shape[2]))(x)\n",
    "    \n",
    "    for i in range(2):\n",
    "        x = Conv1DTranspose(input_tensor=x, filters=filters,\n",
    "                            kernel_size=2, padding='same', last=False)\n",
    "       \n",
    "       \n",
    "    #decoded = LSTM(n_features, return_sequences=True)(x)\n",
    "    \n",
    "    output = Dense(n_features)(x)\n",
    "    \n",
    "    encoder = Model(inputs, encoded)\n",
    "    decoder = Model(latent_inputs, output)\n",
    "    \n",
    "    output = decoder(encoder.output)\n",
    "    sequence_autoencoder = Model(inputs, output)\n",
    "    sequence_autoencoder.summary()\n",
    "    sequence_autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return sequence_autoencoder, encoder\n",
    "\n",
    "\n",
    "model, encoder = create_ae()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 900, 4)\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.1417\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0598\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0421\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0276\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 0.0182\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 3s 8ms/step - loss: 0.0124A: 0s - loss: 0\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0078\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0044\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0021\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0010\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 5.5125e-04\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.3546e-04\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 2.2851e-04\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 1.6889e-04\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 1.3326e-04\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 1.1236e-04\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 9.7787e-05\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 8.5902e-05\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 7.4733e-05\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 6.7281e-05\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 6.1806e-05\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 5.8846e-05\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 5.7112e-05\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 5.7742e-05\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 5.4411e-05\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.9999e-05\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 6.0144e-05\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 5.6399e-05\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.9842e-05\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.3011e-05\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.5442e-05\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 4.2452e-05\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 4.1059e-05\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 3.9755e-05\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 4.2694e-05\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.6145e-05\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.4759e-05\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.3770e-05\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 4.1466e-05\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 4.0082e-05\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.2532e-05\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.3055e-05\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 4.4557e-05\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 4.9807e-05\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.6589e-05\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 5.4307e-05\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 4.5145e-05\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.2790e-05\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 5.5092e-05\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.6240e-05\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 4s 11ms/step - loss: 5.4435e-05\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 7.7029e-05\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 4.2972e-05\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 4.3984e-05\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 5.5582e-05\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 7.8926e-05\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 1.0715e-04\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 1.9684e-04\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 1.4974e-04\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 8.3840e-05\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 9.4311e-05\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 5.0229e-05\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 3.7907e-05\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 3s 8ms/step - loss: 3.5975e-05\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 3s 6ms/step - loss: 3.4579e-05\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.5411e-05\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.4262e-05\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.4742e-05\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.3629e-05\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.3968e-05\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.4600e-05\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.3566e-05\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.4189e-05\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.5620e-05\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.6356e-05\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.4593e-05\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.5251e-05\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 3.7766e-05\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.5726e-05\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 3.4368e-05\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 3.5201e-05\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 5ms/step - loss: 3.4328e-05\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.4713e-05\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.4896e-05\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 3.7618e-05\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 4.0183e-05\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.4566e-05\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.4229e-05\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.3273e-05\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.3193e-05\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 3.3978e-05\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.4878e-05\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 3.6416e-05\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 3.7636e-05\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 3.7568e-05\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 3.8838e-05\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 4.0957e-05\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 1.4658e-04A: 1s - loss: \n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 3.2748e-04\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 2s 6ms/step - loss: 1.5866e-04\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(train_matrix.shape)\n",
    "    model.fit(train_matrix,train_matrix, epochs=100, verbose=1)\n",
    "    model.save_weights(\"Models/Weights/VAE_CONV_BATCH_400_cycles_ONLY_NOM_MATRIX_LEN.hdf5\")\n",
    "\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_weights(\"Models/Weights/VAE_CONV_BATCH_1000_diff_len_980_MATRIX_LEN.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 10) (400, 10) (400, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "encodings = encoder.predict(train_matrix)\n",
    "enc_mean, enc_var, z_enc = encodings, encodings, encodings\n",
    "print(enc_mean.shape, enc_var.shape, z_enc.shape)\n",
    "\n",
    "\n",
    "def return_mask(num, labs):\n",
    "    arg = np.squeeze(np.argwhere(labs == num))\n",
    "    return arg\n",
    "\n",
    "\n",
    "masks = [return_mask(num, np.array(labels))[:, 0] for num in range(0, 9)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 3)\n",
      "[0.723073   0.19396296 0.06001483]\n",
      "(400, 3)\n",
      "[0.723073   0.19396296 0.06001483]\n",
      "(400, 3)\n",
      "[0.723073   0.19396296 0.06001483]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "\n",
    "\n",
    "def plot_pca(title, i): \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    markers = ['o', 'o', 'o', 'o', '^', '^', '^', '^', '^', '^']\n",
    "    for index, mask in enumerate(masks):\n",
    "        ax.scatter(principalComponents[:, 0][mask], \n",
    "                   principalComponents[:, 1][mask],  \n",
    "                   principalComponents[:, 2][mask], marker=markers[index])\n",
    "    ax.scatter(unseen_encoding[:,0],\n",
    "               unseen_encoding[:,1],\n",
    "               unseen_encoding[:,2])\n",
    "    plt.legend(labels=np.arange(0, 9))\n",
    "    plt.title(str(title))\n",
    "    plt.show()\n",
    "    \n",
    "    for mask in masks:\n",
    "        plt.scatter(x=principalComponents[:, 0][mask], \n",
    "                    y=principalComponents[:, 1][mask],\n",
    "                    alpha=0.5)\n",
    "    for mask in unseen_mask:\n",
    "        plt.scatter(unseen_encoding[:,0][mask],\n",
    "               unseen_encoding[:,1][mask])\n",
    "        #break\n",
    "    \n",
    "    plt.legend(labels=np.arange(0, 9))\n",
    "    plt.title(str(title))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "enc_list = [enc_mean, enc_var, z_enc]\n",
    "titles = [\"MEAN\",\"LOG_VAR\",\"SAMPLED\"]\n",
    "for i,enc in enumerate(enc_list):\n",
    "    scaler = StandardScaler()\n",
    "    enc_input = scaler.fit_transform(enc) \n",
    "    pca = PCA(3)\n",
    "    principalComponents = pca.fit_transform(enc_input)\n",
    "    print(principalComponents.shape)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    plot_pca('Sequences'+titles[i], 0)\n",
    "    \n",
    "    principalComponents = enc\n",
    "    plot_pca('Sequences_Not_Pca'+titles[i], 0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "tsne_obj= tsne.fit_transform(enc_input)\n",
    "print(tsne_obj.shape)\n",
    "\n",
    "for mask in masks:\n",
    "    plt.scatter(x=tsne_obj[:, 0][mask], \n",
    "                y=tsne_obj[:, 1][mask],\n",
    "                alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "(400, 900, 4)\n",
      "(400, 900, 4)\n",
      "400 (10,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "unseen_labs = pd.read_csv(\"Data/Boat_nominal_data/Boat_unseen_labels_mix.csv\")\n",
    "unseen_labs = unseen_labs.drop(columns=\"Unnamed: 0\") \n",
    "unseen_labs = np.array(unseen_labs)\n",
    "unseen_mask = [return_mask(num, np.array(unseen_labs))[:, 0] for num in range(0, 9)]\n",
    "\n",
    "unseen_sequences_matrix = prepare_training(\"Mix_sequences_var_length/run_unseen^\", \n",
    "                                           n_runs=400)\n",
    "print(unseen_sequences_matrix.shape)\n",
    "unseen_encoding = encoder.predict(unseen_sequences_matrix)\n",
    "print(len(unseen_encoding), unseen_encoding[0].shape)\n",
    "\n",
    "#print(unseen_encoding[0][:, 0])\n",
    "# def plot_unseen():\n",
    "#     #CALLS PLOT_PCA with new points\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv\n",
    "import numpy as np\n",
    "x, y, z = unseen_encoding[:,0], unseen_encoding[:,1], unseen_encoding[:,2]\n",
    "\n",
    "for mask in unseen_mask:\n",
    "    ipv.scatter(x[mask], y[mask], z[mask], size=0.3, marker=\"sphere\")\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
