{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "(400, 900, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Conv1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "n_features = 4 \n",
    "labels = pd.read_csv(\"Data/Boat_nominal_data/Boat_mix_len_labels.csv\")\n",
    "labels = labels.drop(columns=\"Unnamed: 0\") \n",
    "labels = np.array(labels)\n",
    "max_len = 0\n",
    "\n",
    "\n",
    "def prepare_training(path, n_runs):\n",
    "    def get_max_len(sequence_list):\n",
    "        m_len = 0\n",
    "        for seq in sequence_list:\n",
    "            if len(seq) > m_len:\n",
    "                m_len = len(seq)\n",
    "        max_len = m_len\n",
    "        print(max_len)\n",
    "        return m_len\n",
    "    \n",
    "    \n",
    "    def construct_matrix(sequence_list):\n",
    "        max_len = get_max_len(sequence_list)\n",
    "        train_matrix = np.zeros(shape=(n_runs, max_len, n_features))\n",
    "        for index,run in enumerate(sequence_list):\n",
    "            train_matrix[index, :len(run)] = run\n",
    "        print(train_matrix.shape)\n",
    "        return train_matrix\n",
    "        \n",
    "        \n",
    "    def stadard_sequences(sequences):\n",
    "        for i, seq in enumerate(sequences):\n",
    "            sequences[i] = MinMaxScaler(feature_range=[0, 1]).fit_transform(seq)\n",
    "        return sequences       \n",
    "    \n",
    "    def read_sequences():\n",
    "        run_list_mix = []\n",
    "        for index in range(n_runs):\n",
    "            run_csv = pd.read_csv(path+str(index))\n",
    "            run_csv = run_csv.drop(columns=['Unnamed: 0'])\n",
    "            run_list_mix.append(run_csv)\n",
    "        stand_sequences = stadard_sequences(run_list_mix)\n",
    "        padded_matrix = construct_matrix(stand_sequences)\n",
    "        return padded_matrix\n",
    "    \n",
    "    return read_sequences()\n",
    "    \n",
    "\n",
    "train_matrix = prepare_training(\"Mix_sequences_var_length/run^\", n_runs=400) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 900, 4)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 450, 50)           450       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 225, 50)           5050      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 11250)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                112510    \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 900, 4)            134054    \n",
      "=================================================================\n",
      "Total params: 252,064\n",
      "Trainable params: 252,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Luca\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Input, LSTM, RepeatVector, Conv2DTranspose\n",
    "from keras.losses import mse\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "filters = 50\n",
    "intermediate_dimension = 30 \n",
    "latent_dim = 10\n",
    "\n",
    "def Conv1DTranspose(input_tensor, filters, kernel_size,last, strides=2, padding='same'):\n",
    "        if last:\n",
    "            activation = 'linear'\n",
    "        else:\n",
    "            activation = 'relu'\n",
    "        x = Lambda(lambda x: K.expand_dims(x, axis=2))(input_tensor)\n",
    "        x = Conv2DTranspose(filters=filters, kernel_size=(kernel_size, 1), \n",
    "                            strides=(strides, 1), padding=padding,\n",
    "                            activation=activation)(x)\n",
    "        x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def repeat(x):\n",
    "    steps_matrix = K.ones_like(x[0][:, :, :1])\n",
    "    latent_matrix = K.expand_dims(x[1], axis=1)\n",
    "    return K.batch_dot(steps_matrix, latent_matrix)\n",
    "\n",
    "\n",
    "def create_vae():\n",
    "    print(max_len)\n",
    "    inputs = Input(shape=(180, n_features))\n",
    "    x = inputs\n",
    "    \n",
    "    for i in range(2):\n",
    "        x = Conv1D(filters=filters,\n",
    "                   kernel_size=2,\n",
    "                   strides=2,\n",
    "                   padding='same')(x)\n",
    "    \n",
    "    shape = K.int_shape(x)\n",
    "    \n",
    "    #before_flattening = LSTM(intermediate_dimension, return_sequences=True)(x)\n",
    "    #x_flat = LSTM(intermediate_dimension)(before_flattening)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    \n",
    "    embeddings = Dense(latent_dim)(x)\n",
    "    \n",
    "    z_mean = Dense(latent_dim, name='z_mean',)(embeddings)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(embeddings)\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    \n",
    "    latent_inputs = Input(shape=(latent_dim,), name='latent_inputs')\n",
    "    #x = Lambda(repeat)([before_flattening, z])\n",
    "    x = Dense(shape[1]*shape[2])(latent_inputs)\n",
    "    x = Reshape((shape[1],shape[2]))(x)\n",
    "    \n",
    "    \n",
    "    print(K.int_shape(x))\n",
    "    for i in range(2):\n",
    "        x = Conv1DTranspose(input_tensor=x,\n",
    "                            filters=filters,\n",
    "                            kernel_size=2,\n",
    "                            last=False,\n",
    "                            padding='same')\n",
    "    \n",
    "    \n",
    "    outputs = Conv1DTranspose(input_tensor=x,\n",
    "                              filters=n_features,\n",
    "                              kernel_size=2,\n",
    "                              strides=1,\n",
    "                              last=True,\n",
    "                              padding='same')\n",
    "    \n",
    "    encoder.summary()\n",
    "    decoder = Model(latent_inputs, outputs)\n",
    "    decoder.summary()\n",
    "    outputs = decoder(encoder.outputs[2])\n",
    "    reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "    #reconstruction_loss *= sequence_length*4\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), \n",
    "                             axis=-1)\n",
    "    loss = K.mean(reconstruction_loss+0*kl_loss)\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "    vae.add_loss(loss)\n",
    "    #vae.summary()\n",
    "    vae.compile(optimizer='adam')\n",
    "    return vae, encoder\n",
    "\n",
    "\n",
    "def create_ae():\n",
    "    inputs = Input(shape=(900, n_features))\n",
    "    x = inputs\n",
    "    for i in range(2):\n",
    "        x = Conv1D(filters=filters, kernel_size=2, strides=2,\n",
    "                   activation='relu', padding='same')(x)\n",
    "    shape = K.int_shape(x)   \n",
    "    before_flattening = x #LSTM(filters, return_sequences=True)(x)   \n",
    "    #encoded = LSTM(intermediate_dimension)(before_flattening)\n",
    "    encoded = Flatten()(x)\n",
    "    encoded = Dense(latent_dim)(encoded)\n",
    "    #x = Lambda(repeat)([before_flattening, encoded])\n",
    "    \n",
    "    latent_inputs = Input(shape=(latent_dim,), name='latent_inputs')\n",
    "    x = Dense(shape[1]*shape[2])(latent_inputs)\n",
    "    x = Reshape((shape[1],shape[2]))(x)\n",
    "    \n",
    "    for i in range(2):\n",
    "        x = Conv1DTranspose(input_tensor=x, filters=filters,\n",
    "                            kernel_size=2, padding='same', last=False)\n",
    "       \n",
    "       \n",
    "    #decoded = LSTM(n_features, return_sequences=True)(x)\n",
    "    \n",
    "    output = Dense(n_features)(x)\n",
    "    \n",
    "    encoder = Model(inputs, encoded)\n",
    "    decoder = Model(latent_inputs, output)\n",
    "    \n",
    "    output = decoder(encoder.output)\n",
    "    sequence_autoencoder = Model(inputs, output)\n",
    "    sequence_autoencoder.summary()\n",
    "    sequence_autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return sequence_autoencoder, encoder\n",
    "\n",
    "\n",
    "model, encoder = create_ae()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 900, 4)\n",
      "Epoch 1/100\n",
      "400/400 [==============================] - 6s 15ms/step - loss: 0.1685\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 6s 16ms/step - loss: 0.0714\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.0469\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.0404\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 0.0339\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 0.0223\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 7s 18ms/step - loss: 0.0158\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 8s 21ms/step - loss: 0.0116\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 0.0079\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 0.0046\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 0.0023\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 0.0012\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 7.6210e-04\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 5.8843e-04\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 4.9655e-04\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 4.4769e-04\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 4.1465e-04\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 4.0016e-04\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.8592e-04\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.7391e-04\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.6184e-04\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.5592e-04\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.4468e-04\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.4857e-04\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.5497e-04\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.5217e-04\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.4136e-04\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.3505e-04\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.3986e-04\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 3.5498e-04\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.3650e-04\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2939e-04\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2869e-04\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.3073e-04\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2974e-04\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 3.3156e-04\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.3727e-04\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.3661e-04\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.3478e-04\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.3358e-04\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.3910e-04\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.4064e-04\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 3.3702e-04\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 3.3028e-04\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2720e-04\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 4s 10ms/step - loss: 3.2304e-04\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.3080e-04\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.4184e-04\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.4730e-04\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.7873e-04\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.9185e-04\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.5232e-04\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.6484e-04\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.6426e-04\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2986e-04\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2732e-04\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2876e-04\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.2812e-04\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2209e-04\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.2763e-04\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2565e-04\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.2392e-04\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2634e-04\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2832e-04\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.1999e-04\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.2533e-04\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.3412e-04\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.2586e-04\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.1417e-04\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.2280e-04\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.2566e-04\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.2084e-04\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.1256e-04\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.1052e-04\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.1206e-04\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.1197e-04\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.1047e-04\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.1250e-04\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 3s 9ms/step - loss: 3.2477e-04\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.1754e-04\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 2.9944e-04\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 4s 9ms/step - loss: 3.0353e-04\n",
      "Epoch 83/100\n",
      "128/400 [========>.....................] - ETA: 2s - loss: 3.4254e-04"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def train():\n",
    "    print(train_matrix.shape)\n",
    "    model.fit(train_matrix,train_matrix, epochs=100, verbose=1)\n",
    "    model.save_weights(\"Models/Weights/VAE_CONV_BATCH_400_cycles_MATRIX_LEN.hdf5\")\n",
    "\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_weights(\"Models/Weights/VAE_CONV_BATCH_1000_diff_len_980_MATRIX_LEN.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 10) (400, 10) (400, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "encodings = encoder.predict(train_matrix)\n",
    "enc_mean, enc_var, z_enc = encodings, encodings, encodings\n",
    "print(enc_mean.shape, enc_var.shape, z_enc.shape)\n",
    "\n",
    "\n",
    "def return_mask(num, labs):\n",
    "    arg = np.squeeze(np.argwhere(labs == num))\n",
    "    return arg\n",
    "\n",
    "\n",
    "masks = [return_mask(num, np.array(labels))[:, 0] for num in range(0, 9)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 3)\n",
      "[0.723073   0.19396296 0.06001483]\n",
      "(400, 3)\n",
      "[0.723073   0.19396296 0.06001483]\n",
      "(400, 3)\n",
      "[0.723073   0.19396296 0.06001483]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "\n",
    "\n",
    "def plot_pca(title, i): \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    markers = ['o', 'o', 'o', 'o', '^', '^', '^', '^', '^', '^']\n",
    "    for index, mask in enumerate(masks):\n",
    "        ax.scatter(principalComponents[:, 0][mask], \n",
    "                   principalComponents[:, 1][mask],  \n",
    "                   principalComponents[:, 2][mask], marker=markers[index])\n",
    "    ax.scatter(unseen_encoding[:,0],\n",
    "               unseen_encoding[:,1],\n",
    "               unseen_encoding[:,2])\n",
    "    plt.legend(labels=np.arange(0, 9))\n",
    "    plt.title(str(title))\n",
    "    plt.show()\n",
    "    \n",
    "    for mask in masks:\n",
    "        plt.scatter(x=principalComponents[:, 0][mask], \n",
    "                    y=principalComponents[:, 1][mask],\n",
    "                    alpha=0.5)\n",
    "    for mask in unseen_mask:\n",
    "        plt.scatter(unseen_encoding[:,0][mask],\n",
    "               unseen_encoding[:,1][mask])\n",
    "        #break\n",
    "    \n",
    "    plt.legend(labels=np.arange(0, 9))\n",
    "    plt.title(str(title))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "enc_list = [enc_mean, enc_var, z_enc]\n",
    "titles = [\"MEAN\",\"LOG_VAR\",\"SAMPLED\"]\n",
    "for i,enc in enumerate(enc_list):\n",
    "    scaler = StandardScaler()\n",
    "    enc_input = scaler.fit_transform(enc) \n",
    "    pca = PCA(3)\n",
    "    principalComponents = pca.fit_transform(enc_input)\n",
    "    print(principalComponents.shape)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    plot_pca('Sequences'+titles[i], 0)\n",
    "    \n",
    "    principalComponents = enc\n",
    "    plot_pca('Sequences_Not_Pca'+titles[i], 0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "tsne_obj= tsne.fit_transform(enc_input)\n",
    "print(tsne_obj.shape)\n",
    "\n",
    "for mask in masks:\n",
    "    plt.scatter(x=tsne_obj[:, 0][mask], \n",
    "                y=tsne_obj[:, 1][mask],\n",
    "                alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "(400, 900, 4)\n",
      "(400, 900, 4)\n",
      "400 (10,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "unseen_labs = pd.read_csv(\"Data/Boat_nominal_data/Boat_unseen_labels_mix.csv\")\n",
    "unseen_labs = unseen_labs.drop(columns=\"Unnamed: 0\") \n",
    "unseen_labs = np.array(unseen_labs)\n",
    "unseen_mask = [return_mask(num, np.array(unseen_labs))[:, 0] for num in range(0, 9)]\n",
    "\n",
    "unseen_sequences_matrix = prepare_training(\"Mix_sequences_var_length/run_unseen^\", \n",
    "                                           n_runs=400)\n",
    "print(unseen_sequences_matrix.shape)\n",
    "unseen_encoding = encoder.predict(unseen_sequences_matrix)\n",
    "print(len(unseen_encoding), unseen_encoding[0].shape)\n",
    "\n",
    "#print(unseen_encoding[0][:, 0])\n",
    "# def plot_unseen():\n",
    "#     #CALLS PLOT_PCA with new points\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv\n",
    "import numpy as np\n",
    "x, y, z = unseen_encoding[:,0], unseen_encoding[:,1], unseen_encoding[:,2]\n",
    "\n",
    "for mask in unseen_mask:\n",
    "    ipv.scatter(x[mask], y[mask], z[mask], size=0.3, marker=\"sphere\")\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
