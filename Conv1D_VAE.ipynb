{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204800, 4)\n(200, 1024, 4)\n(10, 1024, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Conv1D,UpSampling1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "n_features = 4\n",
    "boat_csv = pd.read_csv(\"Data/Boat_nominal_data/Boat_sequences_mix.csv\")\n",
    "boat_csv = boat_csv.drop(columns=[\"Unnamed: 0\", \"M0C\", \"M1C\", \"Acceleration\",\"Speed\"])\n",
    "scaler = StandardScaler()\n",
    "normal_data = scaler.fit_transform(boat_csv)\n",
    "print(normal_data.shape)\n",
    "\n",
    "boat_val = pd.read_csv(\"Data/Boat_nominal_data/Boat_sequence_mix_val.csv\")\n",
    "boat_val = boat_val.drop(columns=[\"Unnamed: 0\", \"M0C\", \"M1C\", \"Acceleration\",\"Speed\"])\n",
    "scaler = StandardScaler()\n",
    "val_nom_data = scaler.fit_transform(boat_val)\n",
    "\n",
    "def prepare_sequences(data, batch_size):\n",
    "    samples = []\n",
    "    for i in range(0,data.shape[0], batch_size):\n",
    "        sample = data[i:i+batch_size]\t\n",
    "        samples.append(sample)\n",
    "    sequences = np.array(samples)\n",
    "    trainX = np.reshape(sequences, (len(sequences), batch_size, n_features))\n",
    "    return trainX\n",
    "\n",
    "\n",
    "def prepare_data():    \n",
    "    trainX_nominal = prepare_sequences(normal_data,1024) \n",
    "    print(trainX_nominal.shape)\n",
    "    \n",
    "    valX_nominal = prepare_sequences(val_nom_data,1024)\n",
    "    print(valX_nominal.shape)\n",
    "\n",
    "    return trainX_nominal, valX_nominal\n",
    "\n",
    "trainX_nominal, valX_nominal = prepare_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling1D\n",
    "\n",
    "input_shape = (1024, n_features)\n",
    "kernel_size = 3\n",
    "latent_dim = 6\n",
    "use_mse = True   \n",
    "load_weights = False\n",
    "\n",
    "def create_vae(beta):\n",
    "    filters = 64\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    x = inputs\n",
    "    for i in range(4):\n",
    "        x = Conv1D(filters=filters,\n",
    "                   kernel_size=7,\n",
    "                   padding='same')(x)\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        filters = int(filters / 2)\n",
    "    \n",
    "    \n",
    "    shape = K.int_shape(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "    \n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    #encoder.summary()\n",
    "    \n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(shape[1] * shape[2], name='Dense_after_sampling')(latent_inputs)\n",
    "    x = Reshape((shape[1], shape[2]))(x)\n",
    "    filters = filters * 2\n",
    "\n",
    "    for i in range(4):\n",
    "        x = Conv1D(filters=filters,kernel_size=7, padding='same')(x)\n",
    "        x = UpSampling1D(size=2)(x)\n",
    "        filters = filters * 2\n",
    "        \n",
    "        \n",
    "    outputs = Conv1D(filters=n_features, kernel_size=7, padding='same')(x)\n",
    "    \n",
    "    \n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    #decoder.summary()\n",
    "    \n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "    \n",
    "    from keras.losses import mse\n",
    "    \n",
    "    reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
    "    loss = reconstruction_loss + 3*kl_loss\n",
    "    vae.add_loss(loss)\n",
    "    \n",
    "    vae.compile(optimizer='rmsprop', metrics= ['accuracy'])\n",
    "\n",
    "    return (vae, encoder, decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.44444444 1.88888889 2.33333333 2.77777778 3.22222222\n 3.66666667 4.11111111 4.55555556 5.        ]\nCreating VAE with beta= 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VAE with beta= 1.4444444444444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VAE with beta= 1.8888888888888888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VAE with beta= 2.333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VAE with beta= 2.7777777777777777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VAE with beta= 3.2222222222222223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VAE with beta= 3.6666666666666665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VAE with beta= 4.111111111111111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VAE with beta= 4.555555555555555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VAE with beta= 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FITTING Vae with beta = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 10 samples\nEpoch 1/100\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "betas = np.linspace(1,5,10)\n",
    "print(betas)\n",
    "\n",
    "vaes = []\n",
    "for i, beta in enumerate(betas):\n",
    "    print(\"Creating VAE with beta=\", betas[i])\n",
    "    vaes.append(create_vae(beta))\n",
    "for i in range(len(vaes)):\n",
    "    checkpointer = ModelCheckpoint(filepath=\"Models/Weights/Nominal_weights.hdf5\",\n",
    "                                   verbose=1, save_best_only=True)\n",
    "    print(\"FITTING Vae with beta =\", betas[i])\n",
    "    vaes[i][0].fit(x=trainX_nominal, epochs=100, \n",
    "            batch_size=1024,\n",
    "            validation_data=(valX_nominal,None),\n",
    "            callbacks=[checkpointer])\n",
    "    vaes[i][0].load_weights('Models/Weights/Nominal_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "ENCODED ANALYSIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "encodings = []\n",
    "\n",
    "for i in vaes:\n",
    "    encodings.append(i[1].predict(trainX_nominal))\n",
    "\n",
    "\n",
    "labels = pd.read_csv(\"Data/Boat_nominal_data/Boat_mix_labels.csv\")\n",
    "labels = labels.drop(columns=\"Unnamed: 0\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "mat_mask = np.array([labels for i in range(latent_dim)])\n",
    "print(mat_mask.shape)\n",
    "\n",
    "def get_neuron_values(encoding):\n",
    "    neurons_m = []\n",
    "    neurons_var = []\n",
    "    for i in range(latent_dim):\n",
    "        neurons_m.append(encoding[0][:, i])\n",
    "        neurons_var.append(encoding[1][:, i])\n",
    "    \n",
    "    for i in range(latent_dim):\n",
    "        plt.plot(neurons_m[i])\n",
    "    plt.show()\n",
    "    \n",
    "    return (neurons_m, neurons_var)\n",
    "\n",
    "neuron_values = []\n",
    "for i in encodings:\n",
    " neuron_values.append(get_neuron_values(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def nominal_parameters(n_m, n_var):\n",
    "    neuron_avg_nom = np.ma.array(n_m, mask=np.logical_not(mat_mask))\n",
    "    neuron_avg_nom = np.mean(neuron_avg_nom, axis=1)\n",
    "    neuron_var_nom = np.ma.array(n_var, mask=np.logical_not(mat_mask))\n",
    "    neuron_var_nom = np.mean(neuron_var_nom, axis=1)\n",
    "     \n",
    "    neuron_avg_anom = np.ma.array(n_m, mask=mat_mask)\n",
    "    neuron_avg_anom = np.mean(neuron_avg_anom, axis=1)\n",
    "    neuron_var_anom = np.ma.array(n_var, mask=mat_mask)\n",
    "    neuron_var_anom = np.mean(neuron_var_anom, axis=1)\n",
    "    \n",
    "    return (neuron_avg_nom, neuron_var_nom, neuron_avg_anom, neuron_var_anom)\n",
    "\n",
    "enc_values = []\n",
    "for i in neuron_values:\n",
    "    enc_values.append(nominal_parameters(i[0], i[1]))\n",
    "print(len(enc_values))"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "DIFFERENCES IN ENCODING OF NOMINAL AND ANOMALOUS BEHAVIOUR VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_difference(avg_n, var_n, avg_an, var_an):\n",
    "    plt.plot(avg_n)\n",
    "    plt.plot(avg_an)\n",
    "    plt.title(\"MEAN\")\n",
    "    plt.show()\n",
    "    \n",
    "    # plt.plot(var_n)\n",
    "    # plt.plot(var_an)\n",
    "    # plt.title(\"STD\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "for i in enc_values:\n",
    "    visualize_difference(i[0], i[1], i[2], i[3])\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "ANALYSIS OF ENCODING AND DECODING OF THE ANOMALOUS RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08811088  0.1255463   0.01701382 -0.08566583  0.1815099  -0.00088965] [ 0.16391774  0.00667964 -0.08026419 -0.01931977 -0.09087465  0.04685581]\n"
     ]
    }
   ],
   "source": [
    "boat_anom_csv = pd.read_csv(\"Data/Boat_anom_sequence.csv\")\n",
    "boat_anom_csv = boat_anom_csv.drop(columns=[\"Unnamed: 0\",\"Speed\", \"Acceleration\",\n",
    "                                            \"M0C\", \"M1C\"])\n",
    "anom_data_norm = scaler.fit_transform(boat_anom_csv)\n",
    "anom_data_norm = np.reshape(anom_data_norm, (1, len(anom_data_norm), n_features))\n",
    "anom_data_encoding = np.squeeze(encoder.predict(anom_data_norm))\n",
    "\n",
    "anom_means = anom_data_encoding[0]\n",
    "anom_var = anom_data_encoding[1]\n",
    "print(anom_means, anom_var)\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "SAMPLING FROM VALUES OF THE LATENT SPACE AND CHANGING ONLY ONE, \n",
    "THEN CHECK THE RECONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_sampled(sample_nom, sample_anom):\n",
    "    plt.plot(sample_nom)    \n",
    "    plt.plot(sample_anom)\n",
    "    plt.title(\"SAMPLED DIFFERENCES\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def reconstruct_from_latent_vectors(latents, model_index):\n",
    "    latents = np.reshape(latents, (1,len(latents)))\n",
    "    run = vaes[model_index][2].predict(latents)\n",
    "    run_df = pd.DataFrame(run[0], columns=boat_csv.columns)\n",
    "    \n",
    "    plt.plot(run_df[\"Lon\"])#, run_df[\"Lat\"])\n",
    "    plt.plot(run_df[\"Lat\"])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "epsilon = np.random.normal(0,1)\n",
    "sample_nominal = enc_values[0][0] + np.exp(0.5*enc_values[0][1])*epsilon\n",
    "sample_anomalous = enc_values[0][2] + np.exp(0.5*enc_values[0][3])*epsilon\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = np.linspace(0,6,50)\n",
    "neur = 0\n",
    "for i in changes:\n",
    "    sample_nominal[neur] = sample_anomalous[neur]*i\n",
    "    #visualize_latent_sampled(sample_nominal, sample_anomalous)\n",
    "    reconstruct_from_latent_vectors(sample_nominal,0)\n",
    "# reconstruct_from_latent_vectors(sample_anomalous)\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "VISUALIZATION of ENCODED VALUES OF MEAN AND SIGMA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "labels = np.array(pd.read_csv(\"Data/Boat_nominal_data/Boat_mix_labels.csv\")['label']) \n",
    "nominals = np.squeeze(np.argwhere(labels==1))\n",
    "anomalous = np.squeeze(np.argwhere(labels==0))\n",
    "print(type(nominals), type(nominals[0]))\n",
    "\n",
    "\n",
    "titles = [\"Mean\", \"Std\", \"Sampled\"]\n",
    "\n",
    "for i in encodings[9]:\n",
    "    scaler = StandardScaler()\n",
    "    enc_input = scaler.fit_transform(i)\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(enc_input)\n",
    "    x_val = []\n",
    "    y_val=[]\n",
    "    for i in range(principalComponents.shape[0]):\n",
    "        x_val.append(principalComponents[i][0])\n",
    "        y_val.append(principalComponents[i][1])\n",
    "    x_val = np.array(x_val)\n",
    "    y_val = np.array(y_val)\n",
    "    \n",
    "    \n",
    "    plt.scatter(x=x_val[nominals],y=y_val[nominals], alpha=0.5)\n",
    "    plt.scatter(x=x_val[anomalous],y=y_val[anomalous], alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne(data, title):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    \n",
    "    tsne_obj= tsne.fit_transform(data)\n",
    "    tsne_df = pd.DataFrame({'X':tsne_obj[:,0],\n",
    "                            'Y':tsne_obj[:,1],\n",
    "                            })\n",
    "    \n",
    "    plt.scatter(x=tsne_df[\"X\"][nominals],\n",
    "                y=tsne_df[\"Y\"][nominals], alpha=0.5)\n",
    "    plt.scatter(x=tsne_df[\"X\"][anomalous],\n",
    "                y=tsne_df[\"Y\"][anomalous], alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return tsne_df\n",
    "\n",
    "for i, encode in enumerate(encodings[9]):\n",
    "    tsne_enc_nom_df = tsne(encode, titles[i])\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Customer Dendograms\")\n",
    "dend_nom = shc.dendrogram(shc.linkage(tsne_enc_nom_df, method='ward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "cluster = AgglomerativeClustering(n_clusters=n_clusters, \n",
    "                                  affinity='euclidean',\n",
    "                                  linkage='ward')\n",
    "cl_nom = cluster.fit_predict(tsne_enc_nom_df)\n",
    "print(cl_nom)\n",
    "\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_nom = []\n",
    "y_val_nom = []\n",
    "\n",
    "x_val_anom = []\n",
    "y_val_anom = []\n",
    "for i, xCoord in enumerate(tsne_enc_nom_df['X']):\n",
    "    if cl_nom[i] == 0:\n",
    "        x_val_nom.append(xCoord)\n",
    "        y_val_nom.append(tsne_enc_nom_df['Y'][i])\n",
    "    else:\n",
    "        x_val_anom.append(xCoord)\n",
    "        y_val_anom.append(tsne_enc_nom_df['Y'][i])\n",
    "\n",
    "plt.scatter(x=x_val_nom,\n",
    "            y=y_val_nom, alpha=0.5)\n",
    "plt.scatter(x=x_val_anom,\n",
    "            y=y_val_anom, alpha=0.5)\n",
    "plt.show()\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
