{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(430992, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72600, 8)\n(13200, 8)\n(656, 656, 8)\n(10, 656, 8)\n(10, 6600, 8)\n(1, 6600, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Conv1D,UpSampling1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "# Setup the network parameters:\n",
    "intermediate_dim = 300\n",
    "latent_dim = 10\n",
    "beta = 5\n",
    "n_sequences = 200\n",
    "n_features = 8\n",
    "\n",
    "boat_csv = pd.read_csv(\"Data/Boat_nominal_data/Boat_nominal_long.csv\")\n",
    "boat_csv = boat_csv.drop(columns=[\"Unnamed: 0\"])\n",
    "scaler = StandardScaler()\n",
    "normal_data = scaler.fit_transform(boat_csv)\n",
    "print(normal_data.shape)\n",
    "\n",
    "boat_val = pd.read_csv(\"Data/Boat_nominal_data/Boat_nom_validation.csv\")\n",
    "boat_val = boat_val.drop(columns=[\"Unnamed: 0\"])\n",
    "scaler = StandardScaler()\n",
    "val_nom_data = scaler.fit_transform(boat_val)\n",
    "\n",
    "\n",
    "boat_anom_csv = pd.read_csv(\"Data/Boat_anomalous_big.csv\")\n",
    "boat_anom_csv= boat_anom_csv.drop(columns=[\"Unnamed: 0\"])    \n",
    "scaler = StandardScaler()\n",
    "anomalous_data = scaler.fit_transform(boat_anom_csv)\n",
    "print(anomalous_data.shape)\n",
    "\n",
    "boat_anom_val_csv = pd.read_csv(\"Data/Boat_anomalous_validation.csv\")\n",
    "boat_anom_val_csv= boat_anom_val_csv.drop(columns=[\"Unnamed: 0\"])    \n",
    "scaler = StandardScaler()\n",
    "anomalous_val_data = scaler.fit_transform(boat_anom_val_csv)\n",
    "print(anomalous_val_data.shape)\n",
    "\n",
    "def prepare_sequences(data, batch_size, interval):\n",
    "    samples = []\n",
    "    for i in range(0,data.shape[0]- batch_size, interval):\n",
    "        sample = data[i:i+batch_size]\t\n",
    "        samples.append(sample)\n",
    "\n",
    "    sequences = np.array(samples)\n",
    "\n",
    "    # Batch size (Number of samples time steps and number of features\n",
    "    trainX = np.reshape(sequences, (len(sequences), batch_size, n_features))\n",
    "\n",
    "    return trainX\n",
    "\n",
    "\n",
    "def prepare_data():    \n",
    "    trainX_nominal = prepare_sequences(normal_data,656,656) \n",
    "    print(trainX_nominal.shape)\n",
    "    \n",
    "    valX_nominal = prepare_sequences(val_nom_data, 656,656)\n",
    "    print(valX_nominal.shape)\n",
    "    \n",
    "    trainX_anomalous = prepare_sequences(anomalous_data,6600,6600)\n",
    "    print(trainX_anomalous.shape)  \n",
    "    \n",
    "    valX_anom = prepare_sequences(anomalous_val_data,6600,6600)\n",
    "    print(valX_anom.shape)\n",
    "\n",
    "    return trainX_nominal, valX_nominal, trainX_anomalous, valX_anom\n",
    "\n",
    "\n",
    "\n",
    "trainX_nominal, valX_nominal, trainX_anomalous, valX_anom = prepare_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_16 (InputLayer)           (None, 656, 8)       0                                            \n__________________________________________________________________________________________________\nconv1d_31 (Conv1D)              (None, 656, 200)     11400       input_16[0][0]                   \n__________________________________________________________________________________________________\nconv1d_32 (Conv1D)              (None, 656, 100)     140100      conv1d_31[0][0]                  \n__________________________________________________________________________________________________\nflatten_16 (Flatten)            (None, 65600)        0           conv1d_32[0][0]                  \n__________________________________________________________________________________________________\nDense_Flatten (Dense)           (None, 200)          13120200    flatten_16[0][0]                 \n__________________________________________________________________________________________________\nz_mean (Dense)                  (None, 10)           2010        Dense_Flatten[0][0]              \n__________________________________________________________________________________________________\nz_var (Dense)                   (None, 10)           2010        Dense_Flatten[0][0]              \n__________________________________________________________________________________________________\nlambda_16 (Lambda)              (None, 10)           0           z_mean[0][0]                     \n                                                                 z_var[0][0]                      \n__________________________________________________________________________________________________\ndense_after_sampling (Dense)    (None, 200)          2200        lambda_16[0][0]                  \n__________________________________________________________________________________________________\ndecoder_output (Dense)          (None, 8)            1608        dense_after_sampling[0][0]       \n__________________________________________________________________________________________________\nreshape_16 (Reshape)            (None, 1, 8)         0           decoder_output[0][0]             \n__________________________________________________________________________________________________\nup_sampling1d_16 (UpSampling1D) (None, 100, 8)       0           reshape_16[0][0]                 \n==================================================================================================\nTotal params: 13,279,528\nTrainable params: 13,279,528\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [656,100,8] vs. [656,656,8]\n\t [[{{node training_15/Adam/gradients/loss_15/up_sampling1d_16_loss/sub_grad/BroadcastGradientArgs}}]]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-fede442f1779>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainX_nominal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainX_nominal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m656\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Lstm_vae_boat_data\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [656,100,8] vs. [656,656,8]\n\t [[{{node training_15/Adam/gradients/loss_15/up_sampling1d_16_loss/sub_grad/BroadcastGradientArgs}}]]"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_conv = 7\n",
    "latent_dim = 10\n",
    "intermediate_dim = 200\n",
    "epsilon_std = 1.0\n",
    "epochs = 5\n",
    "\n",
    "x = Input(batch_shape=(None ,656, n_features))\n",
    "conv_1 = Conv1D(filters=200,\n",
    "                kernel_size=num_conv,\n",
    "                padding='same')(x)\n",
    "conv_2 = Conv1D(filters=100,\n",
    "                kernel_size=num_conv,\n",
    "                padding='same')(conv_1)\n",
    "flat = Flatten()(conv_2) # Since we are passing flat data anyway, we probably don't need this.\n",
    "hidden = Dense(intermediate_dim,name=\"Dense_Flatten\", activation='relu')(flat)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(hidden)\n",
    "z_log_var = Dense(latent_dim, name='z_var')(hidden)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(656,latent_dim),\n",
    "                              mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var- K.square(z_mean) - K.exp(z_log_var)) * epsilon \n",
    "    # the original VAE divides z_log_var with two -- why?\n",
    "\n",
    "\n",
    "z = Lambda(sampling,input_shape=(656,latent_dim), \n",
    "           output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "decoder_h = Dense(intermediate_dim, name='dense_after_sampling')\n",
    "decoder_mean = Dense(n_features,name=\"decoder_output\")\n",
    "\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "x_decoded_mean = Reshape([1,n_features])(x_decoded_mean)\n",
    "x_decoded_mean = UpSampling1D(size=100)(x_decoded_mean)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = n_features * metrics.mean_squared_error(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + 10*kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_mean)\n",
    "vae.compile(optimizer='adam', loss=vae_loss) # 'rmsprop'\n",
    "vae.summary()\n",
    "\n",
    "\n",
    "vae.fit(x=trainX_nominal, y=trainX_nominal, batch_size=656, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
